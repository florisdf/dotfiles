//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-23305415
// Driver 384.111
// Based on LLVM 3.4svn
//

.version 6.0
.target sm_30, texmode_independent
.address_size 64

	// .globl	DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope

.entry DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope(
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_0,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_1,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_2,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_3,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_4,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_5,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_6,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_7,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_8,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_9
)
{
	.reg .pred 	%p<212>;
	.reg .f32 	%f<13>;
	.reg .b32 	%r<409>;
	.reg .f64 	%fd<1862>;
	.reg .b64 	%rd<70>;


	ld.param.u64 	%rd10, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_4];
	ld.param.u64 	%rd11, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_5];
	ld.param.u64 	%rd12, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_6];
	ld.param.u64 	%rd13, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_7];
	ld.param.u64 	%rd14, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_8];
	ld.param.u64 	%rd15, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_9];
	mov.b32	%r150, %envreg3;
	mov.u32 	%r151, %ctaid.x;
	mov.u32 	%r152, %ntid.x;
	mad.lo.s32 	%r1, %r151, %r152, %r150;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r3, %r1, %r2;
	mov.f64 	%fd1782, 0d0000000000000000;
	setp.gt.s32	%p1, %r3, 5;
	mov.f64 	%fd1781, %fd1782;
	mov.f64 	%fd14, %fd1782;
	@%p1 bra 	BB0_6;

	add.s32 	%r365, %r1, %r2;
	mov.f64 	%fd1782, 0d0000000000000000;
	mov.u32 	%r364, 1;
	mov.f64 	%fd1781, %fd1782;
	mov.f64 	%fd14, %fd1782;

BB0_2:
	mul.wide.s32 	%rd16, %r365, 8;
	add.s64 	%rd17, %rd15, %rd16;
	add.s64 	%rd18, %rd14, %rd16;
	ld.global.f64 	%fd4, [%rd18];
	ld.global.f64 	%fd5, [%rd17];
	abs.f64 	%fd265, %fd5;
	setp.gtu.f64	%p2, %fd265, 0d7FF0000000000000;
	@%p2 bra 	BB0_5;

	abs.f64 	%fd266, %fd4;
	setp.gtu.f64	%p3, %fd266, 0d7FF0000000000000;
	@%p3 bra 	BB0_5;

	add.f64 	%fd1782, %fd1782, %fd5;
	add.f64 	%fd1781, %fd1781, %fd4;
	add.f64 	%fd14, %fd14, 0d3FF0000000000000;

BB0_5:
	add.s32 	%r365, %r365, 1;
	setp.lt.s32	%p4, %r365, 6;
	setp.lt.s32	%p5, %r364, 2;
	and.pred  	%p6, %p4, %p5;
	add.s32 	%r364, %r364, 1;
	@%p6 bra 	BB0_2;

BB0_6:
	setp.lt.f64	%p7, %fd14, 0d3FF0000000000000;
	mov.f64 	%fd1802, 0dFFF8000000000000;
	@%p7 bra 	BB0_75;

	setp.eq.f64	%p8, %fd14, 0d3FF0000000000000;
	mov.f64 	%fd1790, 0d3FF0000000000000;
	@%p8 bra 	BB0_38;

	abs.f64 	%fd15, %fd14;
	setp.gtu.f64	%p9, %fd15, 0d7FF0000000000000;
	@%p9 bra 	BB0_37;
	bra.uni 	BB0_9;

BB0_37:
	add.f64 	%fd1790, %fd14, 0dBFF0000000000000;
	bra.uni 	BB0_38;

BB0_9:
	setp.eq.f64	%p10, %fd14, 0d7FF0000000000000;
	mov.f64 	%fd269, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r9}, %fd269;
	}
	@%p10 bra 	BB0_36;
	bra.uni 	BB0_10;

BB0_36:
	setp.gt.s32	%p32, %r9, -1;
	selp.f64	%fd1790, 0d7FF0000000000000, 0d0000000000000000, %p32;
	bra.uni 	BB0_38;

BB0_10:
	and.b32  	%r154, %r9, 2147483647;
	setp.ne.s32	%p11, %r154, 2146435072;
	@%p11 bra 	BB0_12;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r155, %temp}, %fd269;
	}
	setp.eq.s32	%p12, %r155, 0;
	@%p12 bra 	BB0_34;

BB0_12:
	mov.f64 	%fd272, 0d3FE0000000000000;
	mul.rn.f64 	%fd273, %fd272, %fd269;
	cvt.rzi.f64.f64	%fd274, %fd273;
	mov.f64 	%fd275, 0d4000000000000000;
	mul.rn.f64 	%fd276, %fd275, %fd274;
	sub.f64 	%fd277, %fd269, %fd276;
	abs.f64 	%fd16, %fd277;
	setp.eq.f64	%p13, %fd14, 0d0000000000000000;
	@%p13 bra 	BB0_33;
	bra.uni 	BB0_13;

BB0_33:
	setp.eq.f64	%p29, %fd16, 0d3FF0000000000000;
	rcp.rn.f64 	%fd457, %fd14;
	mov.f64 	%fd458, 0d0000000000000000;
	rcp.rn.f64 	%fd459, %fd458;
	selp.f64	%fd1790, %fd457, %fd459, %p29;
	bra.uni 	BB0_38;

BB0_13:
	setp.eq.f64	%p14, %fd14, 0dFFF0000000000000;
	@%p14 bra 	BB0_31;
	bra.uni 	BB0_14;

BB0_31:
	div.rn.f64 	%fd1790, %fd269, %fd14;
	setp.neu.f64	%p28, %fd16, 0d3FF0000000000000;
	@%p28 bra 	BB0_38;

	mov.b64 	 %rd21, %fd1790;
	xor.b64  	%rd22, %rd21, -9223372036854775808;
	mov.b64 	 %fd1790, %rd22;
	bra.uni 	BB0_38;

BB0_34:
	setp.eq.f64	%p30, %fd14, 0dBFF0000000000000;
	@%p30 bra 	BB0_38;

	setp.gt.f64	%p31, %fd15, 0d3FF0000000000000;
	mov.f64 	%fd461, 0d0000000000000000;
	rcp.rn.f64 	%fd462, %fd461;
	selp.f64	%fd1790, 0d0000000000000000, %fd462, %p31;
	bra.uni 	BB0_38;

BB0_14:
	setp.geu.f64	%p15, %fd14, 0d0000000000000000;
	@%p15 bra 	BB0_16;

	cvt.rzi.f64.f64	%fd280, %fd269;
	setp.neu.f64	%p16, %fd280, 0dBFF0000000000000;
	mov.f64 	%fd1790, 0dFFF8000000000000;
	@%p16 bra 	BB0_38;

BB0_16:
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r367}, %fd15; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r366, hi}, %fd15; 
	}
	// inline asm
	bfe.u32 	%r368, %r367, 20, 11;
	setp.ne.s32	%p17, %r368, 0;
	@%p17 bra 	BB0_18;

	mov.f64 	%fd285, 0d4350000000000000;
	mul.rn.f64 	%fd284, %fd15, %fd285;
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r367}, %fd284; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r366, hi}, %fd284; 
	}
	// inline asm
	bfe.u32 	%r160, %r367, 20, 11;
	add.s32 	%r368, %r160, -54;

BB0_18:
	and.b32  	%r163, %r367, -2146435073;
	or.b32  	%r162, %r163, 1072693248;
	// inline asm
	mov.b64 	%fd1786, {%r366, %r162};
	// inline asm
	add.s32 	%r369, %r368, -1023;
	setp.lt.u32	%p18, %r162, 1073127583;
	@%p18 bra 	BB0_20;

	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r164, hi}, %fd1786; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r165}, %fd1786; 
	}
	// inline asm
	add.s32 	%r167, %r165, -1048576;
	// inline asm
	mov.b64 	%fd1786, {%r164, %r167};
	// inline asm
	add.s32 	%r369, %r368, -1022;

BB0_20:
	add.f64 	%fd374, %fd1786, 0d3FF0000000000000;
	rcp.rn.f64 	%fd375, %fd374;
	add.f64 	%fd316, %fd1786, 0dBFF0000000000000;
	mul.rn.f64 	%fd376, %fd316, %fd375;
	add.f64 	%fd364, %fd376, %fd376;
	mul.rn.f64 	%fd312, %fd364, %fd364;
	mov.f64 	%fd291, 0d3EB0F5FF7D2CAFE2;
	mov.f64 	%fd293, 0d3ED0F5D241AD3B5A;
	// inline asm
	fma.rn.f64 	%fd290, %fd291, %fd312, %fd293;
	// inline asm
	mov.f64 	%fd297, 0d3EF3B20A75488A3F;
	// inline asm
	fma.rn.f64 	%fd294, %fd290, %fd312, %fd297;
	// inline asm
	mov.f64 	%fd301, 0d3F1745CDE4FAECD5;
	// inline asm
	fma.rn.f64 	%fd298, %fd294, %fd312, %fd301;
	// inline asm
	mov.f64 	%fd305, 0d3F3C71C7258A578B;
	// inline asm
	fma.rn.f64 	%fd302, %fd298, %fd312, %fd305;
	// inline asm
	mov.f64 	%fd309, 0d3F6249249242B910;
	// inline asm
	fma.rn.f64 	%fd306, %fd302, %fd312, %fd309;
	// inline asm
	mov.f64 	%fd313, 0d3F89999999999DFB;
	// inline asm
	fma.rn.f64 	%fd310, %fd306, %fd312, %fd313;
	// inline asm
	mul.rn.f64 	%fd377, %fd310, %fd312;
	sub.f64 	%fd378, %fd316, %fd364;
	mul.rn.f64 	%fd317, %fd275, %fd378;
	neg.f64 	%fd315, %fd364;
	// inline asm
	fma.rn.f64 	%fd314, %fd315, %fd316, %fd317;
	// inline asm
	mul.rn.f64 	%fd360, %fd375, %fd314;
	add.f64 	%fd380, %fd377, 0d3FB5555555555555;
	mov.f64 	%fd381, 0d3FB5555555555555;
	sub.f64 	%fd382, %fd381, %fd380;
	add.f64 	%fd383, %fd377, %fd382;
	add.f64 	%fd384, %fd383, 0d0000000000000000;
	add.f64 	%fd385, %fd384, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd327, %fd380, %fd385;
	sub.f64 	%fd386, %fd380, %fd327;
	add.f64 	%fd331, %fd385, %fd386;
	mul.rn.f64 	%fd387, %fd327, %fd364;
	neg.f64 	%fd321, %fd387;
	// inline asm
	fma.rn.f64 	%fd318, %fd327, %fd364, %fd321;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd322, %fd331, %fd360, %fd318;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd326, %fd327, %fd360, %fd322;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd330, %fd331, %fd364, %fd326;
	// inline asm
	add.f64 	%fd343, %fd387, %fd330;
	sub.f64 	%fd388, %fd387, %fd343;
	add.f64 	%fd347, %fd330, %fd388;
	mul.rn.f64 	%fd389, %fd343, %fd364;
	neg.f64 	%fd337, %fd389;
	// inline asm
	fma.rn.f64 	%fd334, %fd343, %fd364, %fd337;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd338, %fd347, %fd360, %fd334;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd342, %fd343, %fd360, %fd338;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd346, %fd347, %fd364, %fd342;
	// inline asm
	add.f64 	%fd359, %fd389, %fd346;
	sub.f64 	%fd390, %fd389, %fd359;
	add.f64 	%fd363, %fd346, %fd390;
	mul.rn.f64 	%fd391, %fd359, %fd364;
	neg.f64 	%fd353, %fd391;
	// inline asm
	fma.rn.f64 	%fd350, %fd359, %fd364, %fd353;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd354, %fd363, %fd360, %fd350;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd358, %fd359, %fd360, %fd354;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd362, %fd363, %fd364, %fd358;
	// inline asm
	add.f64 	%fd392, %fd391, %fd362;
	sub.f64 	%fd393, %fd391, %fd392;
	add.f64 	%fd394, %fd362, %fd393;
	add.f64 	%fd395, %fd364, %fd392;
	sub.f64 	%fd396, %fd364, %fd395;
	add.f64 	%fd397, %fd392, %fd396;
	add.f64 	%fd398, %fd394, %fd397;
	add.f64 	%fd399, %fd360, %fd398;
	add.f64 	%fd400, %fd395, %fd399;
	sub.f64 	%fd401, %fd395, %fd400;
	add.f64 	%fd402, %fd399, %fd401;
	cvt.rn.f64.s32	%fd403, %r369;
	mov.f64 	%fd404, 0d3FE62E42FEFA3000;
	mul.rn.f64 	%fd405, %fd403, %fd404;
	mov.f64 	%fd406, 0d3D53DE6AF278ECE6;
	mul.rn.f64 	%fd407, %fd403, %fd406;
	add.f64 	%fd408, %fd405, %fd400;
	sub.f64 	%fd409, %fd405, %fd408;
	add.f64 	%fd410, %fd400, %fd409;
	add.f64 	%fd411, %fd402, %fd410;
	add.f64 	%fd412, %fd407, %fd411;
	add.f64 	%fd367, %fd408, %fd412;
	sub.f64 	%fd413, %fd408, %fd367;
	add.f64 	%fd371, %fd412, %fd413;
	mul.rn.f64 	%fd414, %fd367, %fd269;
	neg.f64 	%fd369, %fd414;
	// inline asm
	fma.rn.f64 	%fd366, %fd367, %fd269, %fd369;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd370, %fd371, %fd269, %fd366;
	// inline asm
	add.f64 	%fd20, %fd414, %fd370;
	sub.f64 	%fd415, %fd414, %fd20;
	add.f64 	%fd21, %fd370, %fd415;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r22}, %fd20;
	}
	mov.b32 	 %f1, %r22;
	abs.f32 	%f2, %f1;
	setp.lt.f32	%p19, %f2, 0f40874911;
	@%p19 bra 	BB0_22;
	bra.uni 	BB0_21;

BB0_22:
	mov.f64 	%fd419, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd420, %fd20, %fd419;
	mov.f64 	%fd421, 0d4338000000000000;
	add.rn.f64 	%fd422, %fd420, %fd421;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r23, %temp}, %fd422;
	}
	mov.f64 	%fd423, 0dC338000000000000;
	add.rn.f64 	%fd424, %fd422, %fd423;
	mov.f64 	%fd425, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd426, %fd424, %fd425, %fd20;
	mov.f64 	%fd427, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd428, %fd424, %fd427, %fd426;
	mov.f64 	%fd429, 0d3E928AF3FCA213EA;
	mov.f64 	%fd430, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd431, %fd430, %fd428, %fd429;
	mov.f64 	%fd432, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd433, %fd431, %fd428, %fd432;
	mov.f64 	%fd434, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd435, %fd433, %fd428, %fd434;
	mov.f64 	%fd436, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd437, %fd435, %fd428, %fd436;
	mov.f64 	%fd438, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd439, %fd437, %fd428, %fd438;
	mov.f64 	%fd440, 0d3F81111111122322;
	fma.rn.f64 	%fd441, %fd439, %fd428, %fd440;
	mov.f64 	%fd442, 0d3FA55555555502A1;
	fma.rn.f64 	%fd443, %fd441, %fd428, %fd442;
	mov.f64 	%fd444, 0d3FC5555555555511;
	fma.rn.f64 	%fd445, %fd443, %fd428, %fd444;
	mov.f64 	%fd446, 0d3FE000000000000B;
	fma.rn.f64 	%fd447, %fd445, %fd428, %fd446;
	mov.f64 	%fd448, 0d3FF0000000000000;
	fma.rn.f64 	%fd449, %fd447, %fd428, %fd448;
	fma.rn.f64 	%fd1787, %fd449, %fd428, %fd448;
	abs.s32 	%r168, %r23;
	setp.lt.s32	%p22, %r168, 1023;
	@%p22 bra 	BB0_24;
	bra.uni 	BB0_23;

BB0_24:
	shl.b32 	%r174, %r23, 20;
	add.s32 	%r370, %r174, 1072693248;
	bra.uni 	BB0_25;

BB0_21:
	setp.lt.s32	%p20, %r22, 0;
	selp.f64	%fd416, 0d0000000000000000, 0d7FF0000000000000, %p20;
	abs.f64 	%fd417, %fd20;
	setp.gtu.f64	%p21, %fd417, 0d7FF0000000000000;
	add.f64 	%fd418, %fd20, %fd20;
	selp.f64	%fd1790, %fd418, %fd416, %p21;
	bra.uni 	BB0_26;

BB0_23:
	add.s32 	%r169, %r23, 2046;
	shl.b32 	%r170, %r169, 19;
	and.b32  	%r171, %r170, -1048576;
	shl.b32 	%r172, %r169, 20;
	sub.s32 	%r370, %r172, %r171;
	mov.u32 	%r173, 0;
	mov.b64 	%fd450, {%r173, %r171};
	mul.f64 	%fd1787, %fd1787, %fd450;

BB0_25:
	mov.u32 	%r175, 0;
	mov.b64 	%fd451, {%r175, %r370};
	mul.f64 	%fd1790, %fd1787, %fd451;

BB0_26:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r176}, %fd1790;
	}
	and.b32  	%r177, %r176, 2147483647;
	setp.ne.s32	%p23, %r177, 2146435072;
	@%p23 bra 	BB0_28;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r178, %temp}, %fd1790;
	}
	setp.eq.s32	%p24, %r178, 0;
	@%p24 bra 	BB0_29;

BB0_28:
	// inline asm
	fma.rn.f64 	%fd1790, %fd1790, %fd21, %fd1790;
	// inline asm

BB0_29:
	setp.neu.f64	%p25, %fd16, 0d3FF0000000000000;
	or.pred  	%p27, %p15, %p25;
	@%p27 bra 	BB0_38;

	mov.b64 	 %rd19, %fd1790;
	xor.b64  	%rd20, %rd19, -9223372036854775808;
	mov.b64 	 %fd1790, %rd20;

BB0_38:
	mul.f64 	%fd38, %fd1782, %fd1790;
	mul.f64 	%fd39, %fd1781, %fd1790;
	mov.f64 	%fd1794, 0d0000000000000000;
	mov.f64 	%fd1793, %fd1794;
	@%p1 bra 	BB0_44;

	mov.b32	%r363, %envreg3;
	mov.u32 	%r362, %ntid.x;
	mov.u32 	%r361, %ctaid.x;
	mad.lo.s32 	%r360, %r361, %r362, %r363;
	mov.u32 	%r359, %tid.x;
	add.s32 	%r372, %r360, %r359;
	mov.f64 	%fd1794, 0d0000000000000000;
	mov.u32 	%r371, 1;
	mov.f64 	%fd1793, %fd1794;

BB0_40:
	mul.wide.s32 	%rd23, %r372, 8;
	add.s64 	%rd24, %rd15, %rd23;
	add.s64 	%rd25, %rd14, %rd23;
	ld.global.f64 	%fd42, [%rd25];
	ld.global.f64 	%fd43, [%rd24];
	abs.f64 	%fd467, %fd43;
	setp.gtu.f64	%p34, %fd467, 0d7FF0000000000000;
	@%p34 bra 	BB0_43;

	abs.f64 	%fd468, %fd42;
	setp.gtu.f64	%p35, %fd468, 0d7FF0000000000000;
	@%p35 bra 	BB0_43;

	sub.f64 	%fd469, %fd43, %fd38;
	sub.f64 	%fd470, %fd42, %fd39;
	fma.rn.f64 	%fd1794, %fd469, %fd470, %fd1794;
	fma.rn.f64 	%fd1793, %fd469, %fd469, %fd1793;

BB0_43:
	add.s32 	%r372, %r372, 1;
	setp.lt.s32	%p36, %r372, 6;
	setp.lt.s32	%p37, %r371, 2;
	and.pred  	%p38, %p36, %p37;
	add.s32 	%r371, %r371, 1;
	@%p38 bra 	BB0_40;

BB0_44:
	mov.f64 	%fd1802, 0dFFF8000000000000;
	setp.eq.f64	%p39, %fd1793, 0d0000000000000000;
	@%p39 bra 	BB0_75;

	setp.eq.f64	%p40, %fd1793, 0d3FF0000000000000;
	mov.f64 	%fd1801, 0d3FF0000000000000;
	@%p40 bra 	BB0_74;

	abs.f64 	%fd50, %fd1793;
	setp.gtu.f64	%p41, %fd50, 0d7FF0000000000000;
	@%p41 bra 	BB0_73;
	bra.uni 	BB0_47;

BB0_73:
	add.f64 	%fd1801, %fd1793, 0dBFF0000000000000;
	bra.uni 	BB0_74;

BB0_47:
	setp.eq.f64	%p42, %fd1793, 0d7FF0000000000000;
	mov.f64 	%fd473, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r32}, %fd473;
	}
	@%p42 bra 	BB0_72;
	bra.uni 	BB0_48;

BB0_72:
	setp.gt.s32	%p62, %r32, -1;
	selp.f64	%fd1801, 0d7FF0000000000000, 0d0000000000000000, %p62;

BB0_74:
	fma.rn.f64 	%fd1802, %fd1794, %fd1801, 0d0000000000000000;

BB0_75:
	mov.f64 	%fd1827, 0d0000000000000000;
	mov.f64 	%fd1803, %fd1827;
	mov.f64 	%fd1810, %fd1827;
	mov.f64 	%fd1811, %fd1827;
	@%p1 bra 	BB0_81;

	mov.b32	%r358, %envreg3;
	mov.u32 	%r357, %ntid.x;
	mov.u32 	%r356, %ctaid.x;
	mad.lo.s32 	%r355, %r356, %r357, %r358;
	mov.u32 	%r354, %tid.x;
	add.s32 	%r379, %r355, %r354;
	mov.f64 	%fd1803, 0d0000000000000000;
	mov.u32 	%r378, 1;
	mov.f64 	%fd1810, %fd1803;
	mov.f64 	%fd1811, %fd1803;

BB0_77:
	mul.wide.s32 	%rd30, %r379, 8;
	add.s64 	%rd31, %rd12, %rd30;
	add.s64 	%rd32, %rd13, %rd30;
	ld.global.f64 	%fd1808, [%rd32];
	ld.global.f64 	%fd1806, [%rd31];
	abs.f64 	%fd670, %fd1806;
	setp.gtu.f64	%p64, %fd670, 0d7FF0000000000000;
	@%p64 bra 	BB0_79;

	abs.f64 	%fd671, %fd1808;
	setp.le.f64	%p65, %fd671, 0d7FF0000000000000;
	@%p65 bra 	BB0_80;

BB0_79:
	add.f64 	%fd1803, %fd1803, 0dBFF0000000000000;
	mov.f64 	%fd1806, 0d0000000000000000;
	mov.f64 	%fd1808, %fd1806;

BB0_80:
	add.f64 	%fd1810, %fd1810, %fd1806;
	add.f64 	%fd1811, %fd1811, %fd1808;
	add.f64 	%fd1803, %fd1803, 0d3FF0000000000000;
	add.s32 	%r379, %r379, 1;
	setp.lt.s32	%p66, %r379, 6;
	setp.lt.s32	%p67, %r378, 2;
	and.pred  	%p68, %p66, %p67;
	add.s32 	%r378, %r378, 1;
	@%p68 bra 	BB0_77;

BB0_81:
	div.rn.f64 	%fd89, %fd1810, %fd1803;
	div.rn.f64 	%fd90, %fd1811, %fd1803;
	mov.u32 	%r381, 0;
	@%p1 bra 	BB0_82;

	mov.u32 	%r380, %r3;
	mov.f64 	%fd1828, %fd1827;
	mov.f64 	%fd1829, %fd1827;

BB0_84:
	mul.wide.s32 	%rd33, %r380, 8;
	add.s64 	%rd34, %rd12, %rd33;
	add.s64 	%rd35, %rd13, %rd33;
	ld.global.f64 	%fd1816, [%rd35];
	ld.global.f64 	%fd1815, [%rd34];
	abs.f64 	%fd680, %fd1815;
	setp.gtu.f64	%p70, %fd680, 0d7FF0000000000000;
	@%p70 bra 	BB0_86;

	abs.f64 	%fd681, %fd1816;
	setp.le.f64	%p71, %fd681, 0d7FF0000000000000;
	@%p71 bra 	BB0_87;

BB0_86:
	mov.f64 	%fd1815, 0d0000000000000000;
	mov.f64 	%fd1816, %fd1815;

BB0_87:
	sub.f64 	%fd98, %fd1816, %fd90;
	sub.f64 	%fd99, %fd1815, %fd89;
	fma.rn.f64 	%fd1829, %fd99, %fd98, %fd1829;
	setp.eq.f64	%p72, %fd99, 0d3FF0000000000000;
	mov.f64 	%fd1821, 0d3FF0000000000000;
	@%p72 bra 	BB0_118;

	abs.f64 	%fd101, %fd99;
	setp.gtu.f64	%p73, %fd101, 0d7FF0000000000000;
	@%p73 bra 	BB0_117;
	bra.uni 	BB0_89;

BB0_117:
	add.f64 	%fd1821, %fd99, 0d4000000000000000;
	bra.uni 	BB0_118;

BB0_89:
	setp.eq.f64	%p74, %fd99, 0d7FF0000000000000;
	mov.f64 	%fd685, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r57}, %fd685;
	}
	@%p74 bra 	BB0_116;
	bra.uni 	BB0_90;

BB0_116:
	setp.gt.s32	%p96, %r57, -1;
	selp.f64	%fd1821, 0d7FF0000000000000, 0d0000000000000000, %p96;
	bra.uni 	BB0_118;

BB0_90:
	and.b32  	%r209, %r57, 2147483647;
	setp.ne.s32	%p75, %r209, 2146435072;
	@%p75 bra 	BB0_92;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r210, %temp}, %fd685;
	}
	setp.eq.s32	%p76, %r210, 0;
	@%p76 bra 	BB0_114;
	bra.uni 	BB0_92;

BB0_114:
	mov.f64 	%fd1821, 0d3FF0000000000000;
	setp.eq.f64	%p94, %fd99, 0dBFF0000000000000;
	@%p94 bra 	BB0_118;

	setp.gt.f64	%p95, %fd101, 0d3FF0000000000000;
	selp.f64	%fd1821, 0d7FF0000000000000, 0d0000000000000000, %p95;
	bra.uni 	BB0_118;

BB0_92:
	mov.f64 	%fd688, 0d3FE0000000000000;
	mul.rn.f64 	%fd689, %fd688, %fd685;
	cvt.rzi.f64.f64	%fd690, %fd689;
	mul.rn.f64 	%fd691, %fd685, %fd690;
	sub.f64 	%fd692, %fd685, %fd691;
	abs.f64 	%fd102, %fd692;
	setp.eq.f64	%p77, %fd99, 0d0000000000000000;
	@%p77 bra 	BB0_113;
	bra.uni 	BB0_93;

BB0_113:
	setp.eq.f64	%p93, %fd102, 0d3FF0000000000000;
	selp.f64	%fd1821, %fd99, 0d0000000000000000, %p93;
	bra.uni 	BB0_118;

BB0_93:
	setp.eq.f64	%p78, %fd99, 0dFFF0000000000000;
	@%p78 bra 	BB0_111;
	bra.uni 	BB0_94;

BB0_111:
	neg.f64 	%fd1821, %fd99;
	setp.neu.f64	%p92, %fd102, 0d3FF0000000000000;
	@%p92 bra 	BB0_118;

	mov.b64 	 %rd38, %fd1821;
	xor.b64  	%rd39, %rd38, -9223372036854775808;
	mov.b64 	 %fd1821, %rd39;
	bra.uni 	BB0_118;

BB0_94:
	setp.geu.f64	%p79, %fd99, 0d0000000000000000;
	@%p79 bra 	BB0_96;

	cvt.rzi.f64.f64	%fd695, %fd685;
	setp.neu.f64	%p80, %fd695, 0d4000000000000000;
	mov.f64 	%fd1821, 0dFFF8000000000000;
	@%p80 bra 	BB0_118;

BB0_96:
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r383}, %fd101; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r382, hi}, %fd101; 
	}
	// inline asm
	bfe.u32 	%r384, %r383, 20, 11;
	setp.ne.s32	%p81, %r384, 0;
	@%p81 bra 	BB0_98;

	mov.f64 	%fd700, 0d4350000000000000;
	mul.rn.f64 	%fd699, %fd101, %fd700;
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r383}, %fd699; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r382, hi}, %fd699; 
	}
	// inline asm
	bfe.u32 	%r215, %r383, 20, 11;
	add.s32 	%r384, %r215, -54;

BB0_98:
	and.b32  	%r218, %r383, -2146435073;
	or.b32  	%r217, %r218, 1072693248;
	// inline asm
	mov.b64 	%fd1817, {%r382, %r217};
	// inline asm
	add.s32 	%r385, %r384, -1023;
	setp.lt.u32	%p82, %r217, 1073127583;
	@%p82 bra 	BB0_100;

	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r219, hi}, %fd1817; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r220}, %fd1817; 
	}
	// inline asm
	add.s32 	%r222, %r220, -1048576;
	// inline asm
	mov.b64 	%fd1817, {%r219, %r222};
	// inline asm
	add.s32 	%r385, %r384, -1022;

BB0_100:
	add.f64 	%fd789, %fd1817, 0d3FF0000000000000;
	rcp.rn.f64 	%fd790, %fd789;
	add.f64 	%fd731, %fd1817, 0dBFF0000000000000;
	mul.rn.f64 	%fd791, %fd731, %fd790;
	add.f64 	%fd779, %fd791, %fd791;
	mul.rn.f64 	%fd727, %fd779, %fd779;
	mov.f64 	%fd706, 0d3EB0F5FF7D2CAFE2;
	mov.f64 	%fd708, 0d3ED0F5D241AD3B5A;
	// inline asm
	fma.rn.f64 	%fd705, %fd706, %fd727, %fd708;
	// inline asm
	mov.f64 	%fd712, 0d3EF3B20A75488A3F;
	// inline asm
	fma.rn.f64 	%fd709, %fd705, %fd727, %fd712;
	// inline asm
	mov.f64 	%fd716, 0d3F1745CDE4FAECD5;
	// inline asm
	fma.rn.f64 	%fd713, %fd709, %fd727, %fd716;
	// inline asm
	mov.f64 	%fd720, 0d3F3C71C7258A578B;
	// inline asm
	fma.rn.f64 	%fd717, %fd713, %fd727, %fd720;
	// inline asm
	mov.f64 	%fd724, 0d3F6249249242B910;
	// inline asm
	fma.rn.f64 	%fd721, %fd717, %fd727, %fd724;
	// inline asm
	mov.f64 	%fd728, 0d3F89999999999DFB;
	// inline asm
	fma.rn.f64 	%fd725, %fd721, %fd727, %fd728;
	// inline asm
	mul.rn.f64 	%fd792, %fd725, %fd727;
	sub.f64 	%fd793, %fd731, %fd779;
	mul.rn.f64 	%fd732, %fd685, %fd793;
	neg.f64 	%fd730, %fd779;
	// inline asm
	fma.rn.f64 	%fd729, %fd730, %fd731, %fd732;
	// inline asm
	mul.rn.f64 	%fd775, %fd790, %fd729;
	add.f64 	%fd794, %fd792, 0d3FB5555555555555;
	mov.f64 	%fd795, 0d3FB5555555555555;
	sub.f64 	%fd796, %fd795, %fd794;
	add.f64 	%fd797, %fd792, %fd796;
	add.f64 	%fd798, %fd797, 0d0000000000000000;
	add.f64 	%fd799, %fd798, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd742, %fd794, %fd799;
	sub.f64 	%fd800, %fd794, %fd742;
	add.f64 	%fd746, %fd799, %fd800;
	mul.rn.f64 	%fd801, %fd742, %fd779;
	neg.f64 	%fd736, %fd801;
	// inline asm
	fma.rn.f64 	%fd733, %fd742, %fd779, %fd736;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd737, %fd746, %fd775, %fd733;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd741, %fd742, %fd775, %fd737;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd745, %fd746, %fd779, %fd741;
	// inline asm
	add.f64 	%fd758, %fd801, %fd745;
	sub.f64 	%fd802, %fd801, %fd758;
	add.f64 	%fd762, %fd745, %fd802;
	mul.rn.f64 	%fd803, %fd758, %fd779;
	neg.f64 	%fd752, %fd803;
	// inline asm
	fma.rn.f64 	%fd749, %fd758, %fd779, %fd752;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd753, %fd762, %fd775, %fd749;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd757, %fd758, %fd775, %fd753;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd761, %fd762, %fd779, %fd757;
	// inline asm
	add.f64 	%fd774, %fd803, %fd761;
	sub.f64 	%fd804, %fd803, %fd774;
	add.f64 	%fd778, %fd761, %fd804;
	mul.rn.f64 	%fd805, %fd774, %fd779;
	neg.f64 	%fd768, %fd805;
	// inline asm
	fma.rn.f64 	%fd765, %fd774, %fd779, %fd768;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd769, %fd778, %fd775, %fd765;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd773, %fd774, %fd775, %fd769;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd777, %fd778, %fd779, %fd773;
	// inline asm
	add.f64 	%fd806, %fd805, %fd777;
	sub.f64 	%fd807, %fd805, %fd806;
	add.f64 	%fd808, %fd777, %fd807;
	add.f64 	%fd809, %fd779, %fd806;
	sub.f64 	%fd810, %fd779, %fd809;
	add.f64 	%fd811, %fd806, %fd810;
	add.f64 	%fd812, %fd808, %fd811;
	add.f64 	%fd813, %fd775, %fd812;
	add.f64 	%fd814, %fd809, %fd813;
	sub.f64 	%fd815, %fd809, %fd814;
	add.f64 	%fd816, %fd813, %fd815;
	cvt.rn.f64.s32	%fd817, %r385;
	mov.f64 	%fd818, 0d3FE62E42FEFA3000;
	mul.rn.f64 	%fd819, %fd817, %fd818;
	mov.f64 	%fd820, 0d3D53DE6AF278ECE6;
	mul.rn.f64 	%fd821, %fd817, %fd820;
	add.f64 	%fd822, %fd819, %fd814;
	sub.f64 	%fd823, %fd819, %fd822;
	add.f64 	%fd824, %fd814, %fd823;
	add.f64 	%fd825, %fd816, %fd824;
	add.f64 	%fd826, %fd821, %fd825;
	add.f64 	%fd782, %fd822, %fd826;
	sub.f64 	%fd827, %fd822, %fd782;
	add.f64 	%fd786, %fd826, %fd827;
	mul.rn.f64 	%fd828, %fd782, %fd685;
	neg.f64 	%fd784, %fd828;
	// inline asm
	fma.rn.f64 	%fd781, %fd782, %fd685, %fd784;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd785, %fd786, %fd685, %fd781;
	// inline asm
	add.f64 	%fd106, %fd828, %fd785;
	sub.f64 	%fd829, %fd828, %fd106;
	add.f64 	%fd107, %fd785, %fd829;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r70}, %fd106;
	}
	mov.b32 	 %f5, %r70;
	abs.f32 	%f6, %f5;
	setp.lt.f32	%p83, %f6, 0f40874911;
	@%p83 bra 	BB0_102;
	bra.uni 	BB0_101;

BB0_102:
	mov.f64 	%fd833, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd834, %fd106, %fd833;
	mov.f64 	%fd835, 0d4338000000000000;
	add.rn.f64 	%fd836, %fd834, %fd835;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r71, %temp}, %fd836;
	}
	mov.f64 	%fd837, 0dC338000000000000;
	add.rn.f64 	%fd838, %fd836, %fd837;
	mov.f64 	%fd839, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd840, %fd838, %fd839, %fd106;
	mov.f64 	%fd841, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd842, %fd838, %fd841, %fd840;
	mov.f64 	%fd843, 0d3E928AF3FCA213EA;
	mov.f64 	%fd844, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd845, %fd844, %fd842, %fd843;
	mov.f64 	%fd846, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd847, %fd845, %fd842, %fd846;
	mov.f64 	%fd848, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd849, %fd847, %fd842, %fd848;
	mov.f64 	%fd850, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd851, %fd849, %fd842, %fd850;
	mov.f64 	%fd852, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd853, %fd851, %fd842, %fd852;
	mov.f64 	%fd854, 0d3F81111111122322;
	fma.rn.f64 	%fd855, %fd853, %fd842, %fd854;
	mov.f64 	%fd856, 0d3FA55555555502A1;
	fma.rn.f64 	%fd857, %fd855, %fd842, %fd856;
	mov.f64 	%fd858, 0d3FC5555555555511;
	fma.rn.f64 	%fd859, %fd857, %fd842, %fd858;
	mov.f64 	%fd860, 0d3FE000000000000B;
	fma.rn.f64 	%fd861, %fd859, %fd842, %fd860;
	mov.f64 	%fd862, 0d3FF0000000000000;
	fma.rn.f64 	%fd863, %fd861, %fd842, %fd862;
	fma.rn.f64 	%fd1818, %fd863, %fd842, %fd862;
	abs.s32 	%r223, %r71;
	setp.lt.s32	%p86, %r223, 1023;
	@%p86 bra 	BB0_104;
	bra.uni 	BB0_103;

BB0_104:
	shl.b32 	%r229, %r71, 20;
	add.s32 	%r386, %r229, 1072693248;
	bra.uni 	BB0_105;

BB0_101:
	setp.lt.s32	%p84, %r70, 0;
	selp.f64	%fd830, 0d0000000000000000, 0d7FF0000000000000, %p84;
	abs.f64 	%fd831, %fd106;
	setp.gtu.f64	%p85, %fd831, 0d7FF0000000000000;
	add.f64 	%fd832, %fd106, %fd106;
	selp.f64	%fd1821, %fd832, %fd830, %p85;
	bra.uni 	BB0_106;

BB0_103:
	add.s32 	%r224, %r71, 2046;
	shl.b32 	%r225, %r224, 19;
	and.b32  	%r226, %r225, -1048576;
	shl.b32 	%r227, %r224, 20;
	sub.s32 	%r386, %r227, %r226;
	mov.u32 	%r228, 0;
	mov.b64 	%fd864, {%r228, %r226};
	mul.f64 	%fd1818, %fd1818, %fd864;

BB0_105:
	mov.u32 	%r230, 0;
	mov.b64 	%fd865, {%r230, %r386};
	mul.f64 	%fd1821, %fd1818, %fd865;

BB0_106:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r231}, %fd1821;
	}
	and.b32  	%r232, %r231, 2147483647;
	setp.ne.s32	%p87, %r232, 2146435072;
	@%p87 bra 	BB0_108;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r233, %temp}, %fd1821;
	}
	setp.eq.s32	%p88, %r233, 0;
	@%p88 bra 	BB0_109;

BB0_108:
	// inline asm
	fma.rn.f64 	%fd1821, %fd1821, %fd107, %fd1821;
	// inline asm

BB0_109:
	setp.neu.f64	%p89, %fd102, 0d3FF0000000000000;
	or.pred  	%p91, %p79, %p89;
	@%p91 bra 	BB0_118;

	mov.b64 	 %rd36, %fd1821;
	xor.b64  	%rd37, %rd36, -9223372036854775808;
	mov.b64 	 %fd1821, %rd37;

BB0_118:
	mov.f64 	%fd1826, 0d3FF0000000000000;
	add.f64 	%fd1827, %fd1827, %fd1821;
	setp.eq.f64	%p97, %fd98, 0d3FF0000000000000;
	@%p97 bra 	BB0_149;

	abs.f64 	%fd125, %fd98;
	setp.gtu.f64	%p98, %fd125, 0d7FF0000000000000;
	@%p98 bra 	BB0_148;
	bra.uni 	BB0_120;

BB0_148:
	add.f64 	%fd1826, %fd98, 0d4000000000000000;
	bra.uni 	BB0_149;

BB0_120:
	setp.eq.f64	%p99, %fd98, 0d7FF0000000000000;
	mov.f64 	%fd872, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r75}, %fd872;
	}
	@%p99 bra 	BB0_147;
	bra.uni 	BB0_121;

BB0_147:
	setp.gt.s32	%p121, %r75, -1;
	selp.f64	%fd1826, 0d7FF0000000000000, 0d0000000000000000, %p121;
	bra.uni 	BB0_149;

BB0_121:
	and.b32  	%r234, %r75, 2147483647;
	setp.ne.s32	%p100, %r234, 2146435072;
	@%p100 bra 	BB0_123;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r235, %temp}, %fd872;
	}
	setp.eq.s32	%p101, %r235, 0;
	@%p101 bra 	BB0_145;
	bra.uni 	BB0_123;

BB0_145:
	mov.f64 	%fd1826, 0d3FF0000000000000;
	setp.eq.f64	%p119, %fd98, 0dBFF0000000000000;
	@%p119 bra 	BB0_149;

	setp.gt.f64	%p120, %fd125, 0d3FF0000000000000;
	selp.f64	%fd1826, 0d7FF0000000000000, 0d0000000000000000, %p120;
	bra.uni 	BB0_149;

BB0_123:
	mov.f64 	%fd875, 0d3FE0000000000000;
	mul.rn.f64 	%fd876, %fd875, %fd872;
	cvt.rzi.f64.f64	%fd877, %fd876;
	mul.rn.f64 	%fd878, %fd872, %fd877;
	sub.f64 	%fd879, %fd872, %fd878;
	abs.f64 	%fd126, %fd879;
	setp.eq.f64	%p102, %fd98, 0d0000000000000000;
	@%p102 bra 	BB0_144;
	bra.uni 	BB0_124;

BB0_144:
	setp.eq.f64	%p118, %fd126, 0d3FF0000000000000;
	selp.f64	%fd1826, %fd98, 0d0000000000000000, %p118;
	bra.uni 	BB0_149;

BB0_124:
	setp.eq.f64	%p103, %fd98, 0dFFF0000000000000;
	@%p103 bra 	BB0_142;
	bra.uni 	BB0_125;

BB0_142:
	neg.f64 	%fd1826, %fd98;
	setp.neu.f64	%p117, %fd126, 0d3FF0000000000000;
	@%p117 bra 	BB0_149;

	mov.b64 	 %rd42, %fd1826;
	xor.b64  	%rd43, %rd42, -9223372036854775808;
	mov.b64 	 %fd1826, %rd43;
	bra.uni 	BB0_149;

BB0_125:
	setp.geu.f64	%p104, %fd98, 0d0000000000000000;
	@%p104 bra 	BB0_127;

	cvt.rzi.f64.f64	%fd882, %fd872;
	setp.neu.f64	%p105, %fd882, 0d4000000000000000;
	mov.f64 	%fd1826, 0dFFF8000000000000;
	@%p105 bra 	BB0_149;

BB0_127:
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r388}, %fd125; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r387, hi}, %fd125; 
	}
	// inline asm
	bfe.u32 	%r389, %r388, 20, 11;
	setp.ne.s32	%p106, %r389, 0;
	@%p106 bra 	BB0_129;

	mov.f64 	%fd887, 0d4350000000000000;
	mul.rn.f64 	%fd886, %fd125, %fd887;
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r388}, %fd886; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r387, hi}, %fd886; 
	}
	// inline asm
	bfe.u32 	%r240, %r388, 20, 11;
	add.s32 	%r389, %r240, -54;

BB0_129:
	and.b32  	%r243, %r388, -2146435073;
	or.b32  	%r242, %r243, 1072693248;
	// inline asm
	mov.b64 	%fd1822, {%r387, %r242};
	// inline asm
	add.s32 	%r390, %r389, -1023;
	setp.lt.u32	%p107, %r242, 1073127583;
	@%p107 bra 	BB0_131;

	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r244, hi}, %fd1822; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r245}, %fd1822; 
	}
	// inline asm
	add.s32 	%r247, %r245, -1048576;
	// inline asm
	mov.b64 	%fd1822, {%r244, %r247};
	// inline asm
	add.s32 	%r390, %r389, -1022;

BB0_131:
	add.f64 	%fd976, %fd1822, 0d3FF0000000000000;
	rcp.rn.f64 	%fd977, %fd976;
	add.f64 	%fd918, %fd1822, 0dBFF0000000000000;
	mul.rn.f64 	%fd978, %fd918, %fd977;
	add.f64 	%fd966, %fd978, %fd978;
	mul.rn.f64 	%fd914, %fd966, %fd966;
	mov.f64 	%fd893, 0d3EB0F5FF7D2CAFE2;
	mov.f64 	%fd895, 0d3ED0F5D241AD3B5A;
	// inline asm
	fma.rn.f64 	%fd892, %fd893, %fd914, %fd895;
	// inline asm
	mov.f64 	%fd899, 0d3EF3B20A75488A3F;
	// inline asm
	fma.rn.f64 	%fd896, %fd892, %fd914, %fd899;
	// inline asm
	mov.f64 	%fd903, 0d3F1745CDE4FAECD5;
	// inline asm
	fma.rn.f64 	%fd900, %fd896, %fd914, %fd903;
	// inline asm
	mov.f64 	%fd907, 0d3F3C71C7258A578B;
	// inline asm
	fma.rn.f64 	%fd904, %fd900, %fd914, %fd907;
	// inline asm
	mov.f64 	%fd911, 0d3F6249249242B910;
	// inline asm
	fma.rn.f64 	%fd908, %fd904, %fd914, %fd911;
	// inline asm
	mov.f64 	%fd915, 0d3F89999999999DFB;
	// inline asm
	fma.rn.f64 	%fd912, %fd908, %fd914, %fd915;
	// inline asm
	mul.rn.f64 	%fd979, %fd912, %fd914;
	sub.f64 	%fd980, %fd918, %fd966;
	mul.rn.f64 	%fd919, %fd872, %fd980;
	neg.f64 	%fd917, %fd966;
	// inline asm
	fma.rn.f64 	%fd916, %fd917, %fd918, %fd919;
	// inline asm
	mul.rn.f64 	%fd962, %fd977, %fd916;
	add.f64 	%fd981, %fd979, 0d3FB5555555555555;
	mov.f64 	%fd982, 0d3FB5555555555555;
	sub.f64 	%fd983, %fd982, %fd981;
	add.f64 	%fd984, %fd979, %fd983;
	add.f64 	%fd985, %fd984, 0d0000000000000000;
	add.f64 	%fd986, %fd985, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd929, %fd981, %fd986;
	sub.f64 	%fd987, %fd981, %fd929;
	add.f64 	%fd933, %fd986, %fd987;
	mul.rn.f64 	%fd988, %fd929, %fd966;
	neg.f64 	%fd923, %fd988;
	// inline asm
	fma.rn.f64 	%fd920, %fd929, %fd966, %fd923;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd924, %fd933, %fd962, %fd920;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd928, %fd929, %fd962, %fd924;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd932, %fd933, %fd966, %fd928;
	// inline asm
	add.f64 	%fd945, %fd988, %fd932;
	sub.f64 	%fd989, %fd988, %fd945;
	add.f64 	%fd949, %fd932, %fd989;
	mul.rn.f64 	%fd990, %fd945, %fd966;
	neg.f64 	%fd939, %fd990;
	// inline asm
	fma.rn.f64 	%fd936, %fd945, %fd966, %fd939;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd940, %fd949, %fd962, %fd936;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd944, %fd945, %fd962, %fd940;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd948, %fd949, %fd966, %fd944;
	// inline asm
	add.f64 	%fd961, %fd990, %fd948;
	sub.f64 	%fd991, %fd990, %fd961;
	add.f64 	%fd965, %fd948, %fd991;
	mul.rn.f64 	%fd992, %fd961, %fd966;
	neg.f64 	%fd955, %fd992;
	// inline asm
	fma.rn.f64 	%fd952, %fd961, %fd966, %fd955;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd956, %fd965, %fd962, %fd952;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd960, %fd961, %fd962, %fd956;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd964, %fd965, %fd966, %fd960;
	// inline asm
	add.f64 	%fd993, %fd992, %fd964;
	sub.f64 	%fd994, %fd992, %fd993;
	add.f64 	%fd995, %fd964, %fd994;
	add.f64 	%fd996, %fd966, %fd993;
	sub.f64 	%fd997, %fd966, %fd996;
	add.f64 	%fd998, %fd993, %fd997;
	add.f64 	%fd999, %fd995, %fd998;
	add.f64 	%fd1000, %fd962, %fd999;
	add.f64 	%fd1001, %fd996, %fd1000;
	sub.f64 	%fd1002, %fd996, %fd1001;
	add.f64 	%fd1003, %fd1000, %fd1002;
	cvt.rn.f64.s32	%fd1004, %r390;
	mov.f64 	%fd1005, 0d3FE62E42FEFA3000;
	mul.rn.f64 	%fd1006, %fd1004, %fd1005;
	mov.f64 	%fd1007, 0d3D53DE6AF278ECE6;
	mul.rn.f64 	%fd1008, %fd1004, %fd1007;
	add.f64 	%fd1009, %fd1006, %fd1001;
	sub.f64 	%fd1010, %fd1006, %fd1009;
	add.f64 	%fd1011, %fd1001, %fd1010;
	add.f64 	%fd1012, %fd1003, %fd1011;
	add.f64 	%fd1013, %fd1008, %fd1012;
	add.f64 	%fd969, %fd1009, %fd1013;
	sub.f64 	%fd1014, %fd1009, %fd969;
	add.f64 	%fd973, %fd1013, %fd1014;
	mul.rn.f64 	%fd1015, %fd969, %fd872;
	neg.f64 	%fd971, %fd1015;
	// inline asm
	fma.rn.f64 	%fd968, %fd969, %fd872, %fd971;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd972, %fd973, %fd872, %fd968;
	// inline asm
	add.f64 	%fd130, %fd1015, %fd972;
	sub.f64 	%fd1016, %fd1015, %fd130;
	add.f64 	%fd131, %fd972, %fd1016;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r88}, %fd130;
	}
	mov.b32 	 %f7, %r88;
	abs.f32 	%f8, %f7;
	setp.lt.f32	%p108, %f8, 0f40874911;
	@%p108 bra 	BB0_133;
	bra.uni 	BB0_132;

BB0_133:
	mov.f64 	%fd1020, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd1021, %fd130, %fd1020;
	mov.f64 	%fd1022, 0d4338000000000000;
	add.rn.f64 	%fd1023, %fd1021, %fd1022;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r89, %temp}, %fd1023;
	}
	mov.f64 	%fd1024, 0dC338000000000000;
	add.rn.f64 	%fd1025, %fd1023, %fd1024;
	mov.f64 	%fd1026, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd1027, %fd1025, %fd1026, %fd130;
	mov.f64 	%fd1028, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd1029, %fd1025, %fd1028, %fd1027;
	mov.f64 	%fd1030, 0d3E928AF3FCA213EA;
	mov.f64 	%fd1031, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd1032, %fd1031, %fd1029, %fd1030;
	mov.f64 	%fd1033, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd1034, %fd1032, %fd1029, %fd1033;
	mov.f64 	%fd1035, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd1036, %fd1034, %fd1029, %fd1035;
	mov.f64 	%fd1037, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd1038, %fd1036, %fd1029, %fd1037;
	mov.f64 	%fd1039, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd1040, %fd1038, %fd1029, %fd1039;
	mov.f64 	%fd1041, 0d3F81111111122322;
	fma.rn.f64 	%fd1042, %fd1040, %fd1029, %fd1041;
	mov.f64 	%fd1043, 0d3FA55555555502A1;
	fma.rn.f64 	%fd1044, %fd1042, %fd1029, %fd1043;
	mov.f64 	%fd1045, 0d3FC5555555555511;
	fma.rn.f64 	%fd1046, %fd1044, %fd1029, %fd1045;
	mov.f64 	%fd1047, 0d3FE000000000000B;
	fma.rn.f64 	%fd1048, %fd1046, %fd1029, %fd1047;
	mov.f64 	%fd1049, 0d3FF0000000000000;
	fma.rn.f64 	%fd1050, %fd1048, %fd1029, %fd1049;
	fma.rn.f64 	%fd1823, %fd1050, %fd1029, %fd1049;
	abs.s32 	%r248, %r89;
	setp.lt.s32	%p111, %r248, 1023;
	@%p111 bra 	BB0_135;
	bra.uni 	BB0_134;

BB0_135:
	shl.b32 	%r254, %r89, 20;
	add.s32 	%r391, %r254, 1072693248;
	bra.uni 	BB0_136;

BB0_132:
	setp.lt.s32	%p109, %r88, 0;
	selp.f64	%fd1017, 0d0000000000000000, 0d7FF0000000000000, %p109;
	abs.f64 	%fd1018, %fd130;
	setp.gtu.f64	%p110, %fd1018, 0d7FF0000000000000;
	add.f64 	%fd1019, %fd130, %fd130;
	selp.f64	%fd1826, %fd1019, %fd1017, %p110;
	bra.uni 	BB0_137;

BB0_134:
	add.s32 	%r249, %r89, 2046;
	shl.b32 	%r250, %r249, 19;
	and.b32  	%r251, %r250, -1048576;
	shl.b32 	%r252, %r249, 20;
	sub.s32 	%r391, %r252, %r251;
	mov.u32 	%r253, 0;
	mov.b64 	%fd1051, {%r253, %r251};
	mul.f64 	%fd1823, %fd1823, %fd1051;

BB0_136:
	mov.u32 	%r255, 0;
	mov.b64 	%fd1052, {%r255, %r391};
	mul.f64 	%fd1826, %fd1823, %fd1052;

BB0_137:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r256}, %fd1826;
	}
	and.b32  	%r257, %r256, 2147483647;
	setp.ne.s32	%p112, %r257, 2146435072;
	@%p112 bra 	BB0_139;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r258, %temp}, %fd1826;
	}
	setp.eq.s32	%p113, %r258, 0;
	@%p113 bra 	BB0_140;

BB0_139:
	// inline asm
	fma.rn.f64 	%fd1826, %fd1826, %fd131, %fd1826;
	// inline asm

BB0_140:
	setp.neu.f64	%p114, %fd126, 0d3FF0000000000000;
	or.pred  	%p116, %p104, %p114;
	@%p116 bra 	BB0_149;

	mov.b64 	 %rd40, %fd1826;
	xor.b64  	%rd41, %rd40, -9223372036854775808;
	mov.b64 	 %fd1826, %rd41;

BB0_149:
	add.f64 	%fd1828, %fd1828, %fd1826;
	add.s32 	%r381, %r381, 1;
	add.s32 	%r380, %r381, %r3;
	setp.lt.s32	%p122, %r380, 6;
	setp.lt.s32	%p123, %r381, 2;
	and.pred  	%p124, %p122, %p123;
	@%p124 bra 	BB0_84;
	bra.uni 	BB0_150;

BB0_82:
	mov.f64 	%fd1828, %fd1827;
	mov.f64 	%fd1829, %fd1827;

BB0_150:
	mov.f64 	%fd1834, 0d0000000000000000;
	mul.f64 	%fd1060, %fd1828, %fd1827;
	sqrt.rn.f64 	%fd1061, %fd1060;
	div.rn.f64 	%fd1062, %fd1829, %fd1061;
	abs.f64 	%fd1063, %fd1062;
	setp.gtu.f64	%p125, %fd1063, 0d7FF0000000000000;
	add.f64 	%fd1064, %fd1062, 0d0000000000000000;
	selp.f64	%fd152, 0dFFF8000000000000, %fd1064, %p125;
	mov.u32 	%r394, 0;
	mov.f64 	%fd1835, %fd1834;
	@%p1 bra 	BB0_156;

	mov.b32	%r353, %envreg3;
	mov.u32 	%r352, %ntid.x;
	mov.u32 	%r351, %ctaid.x;
	mad.lo.s32 	%r350, %r351, %r352, %r353;
	mov.u32 	%r345, %tid.x;
	add.s32 	%r393, %r350, %r345;
	mov.f64 	%fd1834, 0d0000000000000000;
	mov.u32 	%r394, 0;
	mov.u32 	%r392, 1;
	mov.f64 	%fd1835, %fd1834;

BB0_152:
	cvt.s64.s32	%rd1, %r393;
	mul.wide.s32 	%rd44, %r393, 8;
	add.s64 	%rd45, %rd10, %rd44;
	ld.global.f64 	%fd1833, [%rd45];
	abs.f64 	%fd1067, %fd1833;
	setp.gtu.f64	%p127, %fd1067, 0d7FF0000000000000;
	@%p127 bra 	BB0_154;

	shl.b64 	%rd46, %rd1, 3;
	add.s64 	%rd47, %rd11, %rd46;
	ld.global.f64 	%fd1832, [%rd47];
	abs.f64 	%fd1068, %fd1832;
	setp.le.f64	%p128, %fd1068, 0d7FF0000000000000;
	@%p128 bra 	BB0_155;

BB0_154:
	add.s32 	%r394, %r394, -1;
	mov.f64 	%fd1832, 0d0000000000000000;
	mov.f64 	%fd1833, %fd1832;

BB0_155:
	add.f64 	%fd1834, %fd1834, %fd1833;
	add.f64 	%fd1835, %fd1835, %fd1832;
	add.s32 	%r394, %r394, 1;
	cvt.u32.u64	%r263, %rd1;
	add.s32 	%r264, %r263, 1;
	setp.lt.s32	%p129, %r264, 6;
	setp.lt.s32	%p130, %r392, 2;
	and.pred  	%p131, %p130, %p129;
	add.s32 	%r393, %r393, 1;
	add.s32 	%r392, %r392, 1;
	@%p131 bra 	BB0_152;

BB0_156:
	setp.lt.s32	%p132, %r394, 1;
	mov.f64 	%fd1840, 0dFFF8000000000000;
	@%p132 bra 	BB0_164;

	cvt.rn.f64.s32	%fd163, %r394;
	mov.f64 	%fd1839, 0d0000000000000000;
	@%p1 bra 	BB0_163;

	mov.b32	%r349, %envreg3;
	mov.u32 	%r348, %ntid.x;
	mov.u32 	%r347, %ctaid.x;
	mad.lo.s32 	%r346, %r347, %r348, %r349;
	mov.u32 	%r344, %tid.x;
	div.rn.f64 	%fd164, %fd1834, %fd163;
	div.rn.f64 	%fd165, %fd1835, %fd163;
	add.s32 	%r398, %r346, %r344;
	mov.f64 	%fd1839, 0d0000000000000000;
	mov.u32 	%r397, 1;

BB0_159:
	cvt.s64.s32	%rd2, %r398;
	mul.wide.s32 	%rd48, %r398, 8;
	add.s64 	%rd49, %rd10, %rd48;
	ld.global.f64 	%fd167, [%rd49];
	abs.f64 	%fd1074, %fd167;
	setp.gtu.f64	%p134, %fd1074, 0d7FF0000000000000;
	mov.f64 	%fd1837, %fd165;
	mov.f64 	%fd1838, %fd164;
	@%p134 bra 	BB0_162;

	shl.b64 	%rd50, %rd2, 3;
	add.s64 	%rd51, %rd11, %rd50;
	ld.global.f64 	%fd168, [%rd51];
	abs.f64 	%fd1075, %fd168;
	setp.gtu.f64	%p135, %fd1075, 0d7FF0000000000000;
	mov.f64 	%fd1837, %fd165;
	mov.f64 	%fd1838, %fd164;
	@%p135 bra 	BB0_162;

	mov.f64 	%fd1837, %fd168;
	mov.f64 	%fd1838, %fd167;

BB0_162:
	sub.f64 	%fd1076, %fd1837, %fd165;
	sub.f64 	%fd1077, %fd1838, %fd164;
	fma.rn.f64 	%fd1839, %fd1076, %fd1077, %fd1839;
	cvt.u32.u64	%r267, %rd2;
	add.s32 	%r268, %r267, 1;
	setp.lt.s32	%p136, %r268, 6;
	setp.lt.s32	%p137, %r397, 2;
	and.pred  	%p138, %p137, %p136;
	add.s32 	%r398, %r398, 1;
	add.s32 	%r397, %r397, 1;
	@%p138 bra 	BB0_159;

BB0_163:
	div.rn.f64 	%fd1840, %fd1839, %fd163;

BB0_164:
	cvt.s64.s32	%rd3, %r3;
	mov.f64 	%fd177, 0d8000000000000000;
	setp.gt.s32	%p139, %r3, 4;
	@%p139 bra 	BB0_167;

	ld.param.u64 	%rd68, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_3];
	shl.b64 	%rd52, %rd3, 3;
	add.s64 	%rd53, %rd68, %rd52;
	ld.global.f64 	%fd175, [%rd53];
	abs.f64 	%fd1080, %fd175;
	setp.gtu.f64	%p140, %fd1080, 0d7FF0000000000000;
	@%p140 bra 	BB0_167;

	mul.f64 	%fd177, %fd175, 0dBFE6A09E667F3BCC;

BB0_167:
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r269}, %fd177; 
	}
	// inline asm
	setp.lt.s32	%p141, %r269, 1072168960;
	@%p141 bra 	BB0_174;
	bra.uni 	BB0_168;

BB0_174:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r111}, %fd177;
	}
	and.b32  	%r112, %r111, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r113, %temp}, %fd177;
	}
	setp.lt.u32	%p147, %r112, 1072693248;
	@%p147 bra 	BB0_180;
	bra.uni 	BB0_175;

BB0_180:
	mul.f64 	%fd1337, %fd177, %fd177;
	mov.f64 	%fd1338, 0d3E4D5F4BB7A316F6;
	mov.f64 	%fd1339, 0dBE0A83AA3B08FBC2;
	fma.rn.f64 	%fd1340, %fd1339, %fd1337, %fd1338;
	mov.f64 	%fd1341, 0dBE85BDCE301B3CDF;
	fma.rn.f64 	%fd1342, %fd1340, %fd1337, %fd1341;
	mov.f64 	%fd1343, 0d3EBB978FADB81BC9;
	fma.rn.f64 	%fd1344, %fd1342, %fd1337, %fd1343;
	mov.f64 	%fd1345, 0dBEEF4C99D6AE5FB8;
	fma.rn.f64 	%fd1346, %fd1344, %fd1337, %fd1345;
	mov.f64 	%fd1347, 0d3F1F9A2AF549012E;
	fma.rn.f64 	%fd1348, %fd1346, %fd1337, %fd1347;
	mov.f64 	%fd1349, 0dBF4C02DAFC636A47;
	fma.rn.f64 	%fd1350, %fd1348, %fd1337, %fd1349;
	mov.f64 	%fd1351, 0d3F7565BCCF619AC0;
	fma.rn.f64 	%fd1352, %fd1350, %fd1337, %fd1351;
	mov.f64 	%fd1353, 0dBF9B82CE311E321A;
	fma.rn.f64 	%fd1354, %fd1352, %fd1337, %fd1353;
	mov.f64 	%fd1355, 0d3FBCE2F21A04075C;
	fma.rn.f64 	%fd1356, %fd1354, %fd1337, %fd1355;
	mov.f64 	%fd1357, 0dBFD812746B0379B4;
	fma.rn.f64 	%fd1358, %fd1356, %fd1337, %fd1357;
	mov.f64 	%fd1359, 0d3FF20DD750429B6D;
	fma.rn.f64 	%fd1360, %fd1358, %fd1337, %fd1359;
	mul.f64 	%fd1843, %fd177, %fd1360;
	bra.uni 	BB0_181;

BB0_168:
	setp.gt.f64	%p142, %fd177, 0d403B4CCCCCCCCCCD;
	mov.f64 	%fd1844, 0d0000000000000000;
	@%p142 bra 	BB0_182;

	mul.rn.f64 	%fd1087, %fd177, %fd177;
	neg.f64 	%fd1086, %fd1087;
	// inline asm
	fma.rn.f64 	%fd1083, %fd177, %fd177, %fd1086;
	// inline asm
	mov.f64 	%fd1088, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd1842, %fd1086, %fd1088;
	abs.f64 	%fd181, %fd1842;
	setp.ge.f64	%p143, %fd181, 0d4330000000000000;
	@%p143 bra 	BB0_171;

	add.f64 	%fd1089, %fd181, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd1090, %fd1089;
	setp.lt.f64	%p144, %fd181, 0d3FE0000000000000;
	selp.f64	%fd1091, 0d0000000000000000, %fd1090, %p144;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r270, %temp}, %fd1091;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r271}, %fd1091;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r272}, %fd1842;
	}
	and.b32  	%r273, %r272, -2147483648;
	or.b32  	%r274, %r271, %r273;
	mov.b64 	%fd1842, {%r270, %r274};

BB0_171:
	mov.f64 	%fd1094, 0dBFE62E42FEFA39EF;
	// inline asm
	fma.rn.f64 	%fd1092, %fd1842, %fd1094, %fd1086;
	// inline asm
	mov.f64 	%fd1098, 0dBC7ABC9E3B39803F;
	// inline asm
	fma.rn.f64 	%fd1096, %fd1842, %fd1098, %fd1092;
	// inline asm
	cvt.rzi.s32.f64	%r277, %fd1842;
	setp.lt.s32	%p145, %r277, -1020;
	add.s32 	%r278, %r277, 55;
	selp.f64	%fd1153, 0d3C90000000000000, 0d4000000000000000, %p145;
	selp.b32	%r279, %r278, %r277, %p145;
	mov.f64 	%fd1101, 0d3E21F07FCCF58BAD;
	mov.f64 	%fd1103, 0d3E5AFD81DA6C3BAF;
	// inline asm
	fma.rn.f64 	%fd1100, %fd1101, %fd1096, %fd1103;
	// inline asm
	mov.f64 	%fd1107, 0d3E927E55F60F80E6;
	// inline asm
	fma.rn.f64 	%fd1104, %fd1100, %fd1096, %fd1107;
	// inline asm
	mov.f64 	%fd1111, 0d3EC71DDA8F02D666;
	// inline asm
	fma.rn.f64 	%fd1108, %fd1104, %fd1096, %fd1111;
	// inline asm
	mov.f64 	%fd1115, 0d3EFA01A013B894E0;
	// inline asm
	fma.rn.f64 	%fd1112, %fd1108, %fd1096, %fd1115;
	// inline asm
	mov.f64 	%fd1119, 0d3F2A01A01D3AF788;
	// inline asm
	fma.rn.f64 	%fd1116, %fd1112, %fd1096, %fd1119;
	// inline asm
	mov.f64 	%fd1123, 0d3F56C16C16C3A1EC;
	// inline asm
	fma.rn.f64 	%fd1120, %fd1116, %fd1096, %fd1123;
	// inline asm
	mov.f64 	%fd1127, 0d3F81111111109161;
	// inline asm
	fma.rn.f64 	%fd1124, %fd1120, %fd1096, %fd1127;
	// inline asm
	mov.f64 	%fd1131, 0d3FA55555555554C1;
	// inline asm
	fma.rn.f64 	%fd1128, %fd1124, %fd1096, %fd1131;
	// inline asm
	mov.f64 	%fd1135, 0d3FC555555555556F;
	// inline asm
	fma.rn.f64 	%fd1132, %fd1128, %fd1096, %fd1135;
	// inline asm
	mov.f64 	%fd1139, 0d3FE0000000000000;
	// inline asm
	fma.rn.f64 	%fd1136, %fd1132, %fd1096, %fd1139;
	// inline asm
	mul.rn.f64 	%fd1141, %fd1136, %fd1096;
	// inline asm
	fma.rn.f64 	%fd1140, %fd1141, %fd1096, %fd1096;
	// inline asm
	add.s32 	%r280, %r279, 1022;
	shl.b32 	%r276, %r280, 20;
	mov.u32 	%r275, 0;
	// inline asm
	mov.b64 	%fd1144, {%r275, %r276};
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1145, %fd1140, %fd1144, %fd1144;
	// inline asm
	mul.rn.f64 	%fd1152, %fd1145, %fd1153;
	neg.f64 	%fd1151, %fd1152;
	// inline asm
	fma.rn.f64 	%fd1149, %fd1083, %fd1151, %fd1152;
	// inline asm
	setp.lt.s32	%p146, %r269, 1075052544;
	@%p146 bra 	BB0_173;
	bra.uni 	BB0_172;

BB0_173:
	mov.f64 	%fd1201, 0d3FE20DD7452FBC22;
	mov.f64 	%fd1203, 0d401FD453E105E9A2;
	// inline asm
	fma.rn.f64 	%fd1200, %fd1201, %fd177, %fd1203;
	// inline asm
	mov.f64 	%fd1207, 0d404B26245B951FB4;
	// inline asm
	fma.rn.f64 	%fd1204, %fd1200, %fd177, %fd1207;
	// inline asm
	mov.f64 	%fd1211, 0d406C7835DC0F1F49;
	// inline asm
	fma.rn.f64 	%fd1208, %fd1204, %fd177, %fd1211;
	// inline asm
	mov.f64 	%fd1215, 0d4083AFA471E5C766;
	// inline asm
	fma.rn.f64 	%fd1212, %fd1208, %fd177, %fd1215;
	// inline asm
	mov.f64 	%fd1219, 0d4091FB514824F49F;
	// inline asm
	fma.rn.f64 	%fd1216, %fd1212, %fd177, %fd1219;
	// inline asm
	mov.f64 	%fd1223, 0d409450DDEE8272BB;
	// inline asm
	fma.rn.f64 	%fd1220, %fd1216, %fd177, %fd1223;
	// inline asm
	mov.f64 	%fd1227, 0d4086B952E4ECBC50;
	// inline asm
	fma.rn.f64 	%fd1224, %fd1220, %fd177, %fd1227;
	// inline asm
	add.f64 	%fd1229, %fd177, 0d402C35442E99E667;
	mov.f64 	%fd1231, 0d40582F68071A079D;
	// inline asm
	fma.rn.f64 	%fd1228, %fd1229, %fd177, %fd1231;
	// inline asm
	mov.f64 	%fd1235, 0d4079ABD39A029DAA;
	// inline asm
	fma.rn.f64 	%fd1232, %fd1228, %fd177, %fd1235;
	// inline asm
	mov.f64 	%fd1239, 0d409230CA327093FD;
	// inline asm
	fma.rn.f64 	%fd1236, %fd1232, %fd177, %fd1239;
	// inline asm
	mov.f64 	%fd1243, 0d40A174FAB33B54A7;
	// inline asm
	fma.rn.f64 	%fd1240, %fd1236, %fd177, %fd1243;
	// inline asm
	mov.f64 	%fd1247, 0d40A601508230F980;
	// inline asm
	fma.rn.f64 	%fd1244, %fd1240, %fd177, %fd1247;
	// inline asm
	mov.f64 	%fd1251, 0d40A091785EC9331E;
	// inline asm
	fma.rn.f64 	%fd1248, %fd1244, %fd177, %fd1251;
	// inline asm
	mov.f64 	%fd1255, 0d4086B952E52F3622;
	// inline asm
	fma.rn.f64 	%fd1252, %fd1248, %fd177, %fd1255;
	// inline asm
	div.rn.f64 	%fd1256, %fd1224, %fd1252;
	mul.rn.f64 	%fd1844, %fd1256, %fd1149;
	bra.uni 	BB0_182;

BB0_175:
	setp.lt.u32	%p148, %r112, 2146435072;
	@%p148 bra 	BB0_179;
	bra.uni 	BB0_176;

BB0_179:
	mov.b64 	%fd1258, {%r113, %r112};
	mov.f64 	%fd1259, 0dBCF1384CE38C616A;
	mov.f64 	%fd1260, 0d3C8B9C2B870030E8;
	fma.rn.f64 	%fd1261, %fd1260, %fd1258, %fd1259;
	mov.f64 	%fd1262, 0d3D4458AE9746C2FD;
	fma.rn.f64 	%fd1263, %fd1261, %fd1258, %fd1262;
	mov.f64 	%fd1264, 0dBD8E4A44D4F1AB56;
	fma.rn.f64 	%fd1265, %fd1263, %fd1258, %fd1264;
	mov.f64 	%fd1266, 0d3DCFDF15265C58EE;
	fma.rn.f64 	%fd1267, %fd1265, %fd1258, %fd1266;
	mov.f64 	%fd1268, 0dBE0933832F358D51;
	fma.rn.f64 	%fd1269, %fd1267, %fd1258, %fd1268;
	mov.f64 	%fd1270, 0d3E3F136D3F719446;
	fma.rn.f64 	%fd1271, %fd1269, %fd1258, %fd1270;
	mov.f64 	%fd1272, 0dBE6E94C2FE151B3B;
	fma.rn.f64 	%fd1273, %fd1271, %fd1258, %fd1272;
	mov.f64 	%fd1274, 0d3E985A70310EE0A8;
	fma.rn.f64 	%fd1275, %fd1273, %fd1258, %fd1274;
	mov.f64 	%fd1276, 0dBEBF944DA1520B74;
	fma.rn.f64 	%fd1277, %fd1275, %fd1258, %fd1276;
	mov.f64 	%fd1278, 0d3EE09F503825C543;
	fma.rn.f64 	%fd1279, %fd1277, %fd1258, %fd1278;
	mov.f64 	%fd1280, 0dBEFBEEFE9F949E59;
	fma.rn.f64 	%fd1281, %fd1279, %fd1258, %fd1280;
	mov.f64 	%fd1282, 0d3F11D785C6E28857;
	fma.rn.f64 	%fd1283, %fd1281, %fd1258, %fd1282;
	mov.f64 	%fd1284, 0dBF1D866B223048C7;
	fma.rn.f64 	%fd1285, %fd1283, %fd1258, %fd1284;
	mov.f64 	%fd1286, 0d3EF258F0847E8908;
	fma.rn.f64 	%fd1287, %fd1285, %fd1258, %fd1286;
	mov.f64 	%fd1288, 0d3F429CFC58DBB776;
	fma.rn.f64 	%fd1289, %fd1287, %fd1258, %fd1288;
	mov.f64 	%fd1290, 0dBF5BE16D3F71F3C5;
	fma.rn.f64 	%fd1291, %fd1289, %fd1258, %fd1290;
	mov.f64 	%fd1292, 0d3F2E8BDA60326B1A;
	fma.rn.f64 	%fd1293, %fd1291, %fd1258, %fd1292;
	mov.f64 	%fd1294, 0d3F938FB20B0988A6;
	fma.rn.f64 	%fd1295, %fd1293, %fd1258, %fd1294;
	mov.f64 	%fd1296, 0dBFBA4E3A80F64E33;
	fma.rn.f64 	%fd1297, %fd1295, %fd1258, %fd1296;
	mov.f64 	%fd1298, 0dBFE45F3E88093928;
	fma.rn.f64 	%fd1299, %fd1297, %fd1258, %fd1298;
	mov.f64 	%fd1300, 0dBFF20DD599CAEEA0;
	fma.rn.f64 	%fd1301, %fd1299, %fd1258, %fd1300;
	mov.f64 	%fd1302, 0dBE883BE1E31CE133;
	fma.rn.f64 	%fd1303, %fd1301, %fd1258, %fd1302;
	mov.f64 	%fd1304, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd1305, %fd1303, %fd1304;
	mov.f64 	%fd1306, 0d4338000000000000;
	add.rn.f64 	%fd1307, %fd1305, %fd1306;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r285, %temp}, %fd1307;
	}
	mov.f64 	%fd1308, 0dC338000000000000;
	add.rn.f64 	%fd1309, %fd1307, %fd1308;
	mov.f64 	%fd1310, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd1311, %fd1309, %fd1310, %fd1303;
	mov.f64 	%fd1312, 0d3E928AF3FCA213EA;
	mov.f64 	%fd1313, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd1314, %fd1313, %fd1311, %fd1312;
	mov.f64 	%fd1315, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd1316, %fd1314, %fd1311, %fd1315;
	mov.f64 	%fd1317, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd1318, %fd1316, %fd1311, %fd1317;
	mov.f64 	%fd1319, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd1320, %fd1318, %fd1311, %fd1319;
	mov.f64 	%fd1321, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd1322, %fd1320, %fd1311, %fd1321;
	mov.f64 	%fd1323, 0d3F81111111122322;
	fma.rn.f64 	%fd1324, %fd1322, %fd1311, %fd1323;
	mov.f64 	%fd1325, 0d3FA55555555502A1;
	fma.rn.f64 	%fd1326, %fd1324, %fd1311, %fd1325;
	mov.f64 	%fd1327, 0d3FC5555555555511;
	fma.rn.f64 	%fd1328, %fd1326, %fd1311, %fd1327;
	mov.f64 	%fd1329, 0d3FE000000000000B;
	fma.rn.f64 	%fd1330, %fd1328, %fd1311, %fd1329;
	mov.f64 	%fd1331, 0d3FF0000000000000;
	fma.rn.f64 	%fd1332, %fd1330, %fd1311, %fd1331;
	fma.rn.f64 	%fd1333, %fd1332, %fd1311, %fd1331;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r286}, %fd1333;
	}
	shl.b32 	%r287, %r285, 20;
	add.s32 	%r288, %r286, %r287;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r289, %temp}, %fd1333;
	}
	mov.b64 	%fd1334, {%r289, %r288};
	sub.f64 	%fd1335, %fd1331, %fd1334;
	setp.gt.u32	%p152, %r112, 1075294207;
	selp.f64	%fd1336, 0d3FF0000000000000, %fd1335, %p152;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r290, %temp}, %fd1336;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r291}, %fd1336;
	}
	and.b32  	%r292, %r111, -2147483648;
	or.b32  	%r293, %r291, %r292;
	mov.b64 	%fd1843, {%r290, %r293};
	bra.uni 	BB0_181;

BB0_172:
	rcp.rn.f64 	%fd1198, %fd177;
	mul.rn.f64 	%fd1196, %fd1198, %fd1198;
	mov.f64 	%fd1155, 0dC1186DF84479631D;
	mov.f64 	%fd1157, 0d41019A6E9A7FFBB8;
	// inline asm
	fma.rn.f64 	%fd1154, %fd1155, %fd1196, %fd1157;
	// inline asm
	mov.f64 	%fd1161, 0dC0DB040BE3D5CA18;
	// inline asm
	fma.rn.f64 	%fd1158, %fd1154, %fd1196, %fd1161;
	// inline asm
	mov.f64 	%fd1165, 0d40B012760EE009A0;
	// inline asm
	fma.rn.f64 	%fd1162, %fd1158, %fd1196, %fd1165;
	// inline asm
	mov.f64 	%fd1169, 0dC082587AE4008D0E;
	// inline asm
	fma.rn.f64 	%fd1166, %fd1162, %fd1196, %fd1169;
	// inline asm
	mov.f64 	%fd1173, 0d4056DF5D938ACAFE;
	// inline asm
	fma.rn.f64 	%fd1170, %fd1166, %fd1196, %fd1173;
	// inline asm
	mov.f64 	%fd1177, 0dC030A8D46D765681;
	// inline asm
	fma.rn.f64 	%fd1174, %fd1170, %fd1196, %fd1177;
	// inline asm
	mov.f64 	%fd1181, 0d400D9EAE0C665C75;
	// inline asm
	fma.rn.f64 	%fd1178, %fd1174, %fd1196, %fd1181;
	// inline asm
	mov.f64 	%fd1185, 0dBFF0ECF9C8880942;
	// inline asm
	fma.rn.f64 	%fd1182, %fd1178, %fd1196, %fd1185;
	// inline asm
	mov.f64 	%fd1189, 0d3FDB14C2F82A33F7;
	// inline asm
	fma.rn.f64 	%fd1186, %fd1182, %fd1196, %fd1189;
	// inline asm
	mov.f64 	%fd1193, 0dBFD20DD75042844F;
	// inline asm
	fma.rn.f64 	%fd1190, %fd1186, %fd1196, %fd1193;
	// inline asm
	mov.f64 	%fd1197, 0d3FE20DD750429B6B;
	// inline asm
	fma.rn.f64 	%fd1194, %fd1190, %fd1196, %fd1197;
	// inline asm
	mul.rn.f64 	%fd1199, %fd1194, %fd1198;
	mul.rn.f64 	%fd1844, %fd1199, %fd1149;
	bra.uni 	BB0_182;

BB0_176:
	setp.eq.s32	%p149, %r112, 2146435072;
	setp.eq.s32	%p150, %r113, 0;
	and.pred  	%p151, %p149, %p150;
	@%p151 bra 	BB0_178;
	bra.uni 	BB0_177;

BB0_178:
	mov.f64 	%fd1257, 0d3FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r281, %temp}, %fd1257;
	}
	and.b32  	%r282, %r111, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r283}, %fd1257;
	}
	or.b32  	%r284, %r283, %r282;
	mov.b64 	%fd1843, {%r281, %r284};
	bra.uni 	BB0_181;

BB0_177:
	add.f64 	%fd1843, %fd177, %fd177;

BB0_181:
	mov.f64 	%fd1361, 0d3FF0000000000000;
	sub.f64 	%fd1844, %fd1361, %fd1843;

BB0_182:
	ld.param.u64 	%rd66, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_2];
	fma.rn.f64 	%fd194, %fd1844, 0d3FE0000000000000, 0d0000000000000000;
	shl.b64 	%rd54, %rd3, 3;
	add.s64 	%rd4, %rd66, %rd54;
	mov.f64 	%fd1845, 0d0000000000000000;
	mov.f64 	%fd1846, %fd1845;
	@%p139 bra 	BB0_184;

	ld.global.f64 	%fd1364, [%rd4];
	abs.f64 	%fd1365, %fd1364;
	setp.gtu.f64	%p154, %fd1365, 0d7FF0000000000000;
	add.f64 	%fd1366, %fd1364, 0d0000000000000000;
	selp.f64	%fd1845, 0d0000000000000000, %fd1366, %p154;
	selp.f64	%fd1846, 0d0000000000000000, 0d3FF0000000000000, %p154;

BB0_184:
	ld.param.u64 	%rd67, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_1];
	add.s64 	%rd5, %rd67, %rd54;
	@%p139 bra 	BB0_186;

	ld.global.f64 	%fd1367, [%rd5];
	abs.f64 	%fd1368, %fd1367;
	setp.gtu.f64	%p156, %fd1368, 0d7FF0000000000000;
	add.f64 	%fd1369, %fd1845, %fd1367;
	selp.f64	%fd1845, %fd1845, %fd1369, %p156;
	add.f64 	%fd1370, %fd1846, 0d3FF0000000000000;
	selp.f64	%fd1846, %fd1846, %fd1370, %p156;

BB0_186:
	setp.eq.f64	%p157, %fd1846, 0d3FF0000000000000;
	mov.f64 	%fd1853, 0d3FF0000000000000;
	@%p157 bra 	BB0_217;

	abs.f64 	%fd203, %fd1846;
	setp.gtu.f64	%p158, %fd203, 0d7FF0000000000000;
	@%p158 bra 	BB0_216;
	bra.uni 	BB0_188;

BB0_216:
	add.f64 	%fd1853, %fd1846, 0dBFF0000000000000;
	bra.uni 	BB0_217;

BB0_188:
	setp.eq.f64	%p159, %fd1846, 0d7FF0000000000000;
	mov.f64 	%fd1372, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r114}, %fd1372;
	}
	@%p159 bra 	BB0_215;
	bra.uni 	BB0_189;

BB0_215:
	setp.gt.s32	%p181, %r114, -1;
	selp.f64	%fd1853, 0d7FF0000000000000, 0d0000000000000000, %p181;
	bra.uni 	BB0_217;

BB0_189:
	and.b32  	%r294, %r114, 2147483647;
	setp.ne.s32	%p160, %r294, 2146435072;
	@%p160 bra 	BB0_191;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r295, %temp}, %fd1372;
	}
	setp.eq.s32	%p161, %r295, 0;
	@%p161 bra 	BB0_213;

BB0_191:
	mov.f64 	%fd1375, 0d3FE0000000000000;
	mul.rn.f64 	%fd1376, %fd1375, %fd1372;
	cvt.rzi.f64.f64	%fd1377, %fd1376;
	mov.f64 	%fd1378, 0d4000000000000000;
	mul.rn.f64 	%fd1379, %fd1378, %fd1377;
	sub.f64 	%fd1380, %fd1372, %fd1379;
	abs.f64 	%fd204, %fd1380;
	setp.eq.f64	%p162, %fd1846, 0d0000000000000000;
	@%p162 bra 	BB0_212;
	bra.uni 	BB0_192;

BB0_212:
	setp.eq.f64	%p178, %fd204, 0d3FF0000000000000;
	rcp.rn.f64 	%fd1560, %fd1846;
	mov.f64 	%fd1561, 0d0000000000000000;
	rcp.rn.f64 	%fd1562, %fd1561;
	selp.f64	%fd1853, %fd1560, %fd1562, %p178;
	bra.uni 	BB0_217;

BB0_192:
	setp.eq.f64	%p163, %fd1846, 0dFFF0000000000000;
	@%p163 bra 	BB0_210;
	bra.uni 	BB0_193;

BB0_210:
	div.rn.f64 	%fd1853, %fd1372, %fd1846;
	setp.neu.f64	%p177, %fd204, 0d3FF0000000000000;
	@%p177 bra 	BB0_217;

	mov.b64 	 %rd58, %fd1853;
	xor.b64  	%rd59, %rd58, -9223372036854775808;
	mov.b64 	 %fd1853, %rd59;
	bra.uni 	BB0_217;

BB0_48:
	and.b32  	%r181, %r32, 2147483647;
	setp.ne.s32	%p43, %r181, 2146435072;
	@%p43 bra 	BB0_50;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r182, %temp}, %fd473;
	}
	setp.eq.s32	%p44, %r182, 0;
	@%p44 bra 	BB0_70;

BB0_50:
	mov.f64 	%fd476, 0d3FE0000000000000;
	mul.rn.f64 	%fd477, %fd476, %fd473;
	cvt.rzi.f64.f64	%fd478, %fd477;
	mov.f64 	%fd479, 0d4000000000000000;
	mul.rn.f64 	%fd480, %fd479, %fd478;
	sub.f64 	%fd481, %fd473, %fd480;
	abs.f64 	%fd51, %fd481;
	setp.eq.f64	%p45, %fd1793, 0dFFF0000000000000;
	@%p45 bra 	BB0_68;
	bra.uni 	BB0_51;

BB0_68:
	div.rn.f64 	%fd1801, %fd473, %fd1793;
	setp.neu.f64	%p59, %fd51, 0d3FF0000000000000;
	@%p59 bra 	BB0_74;

	mov.b64 	 %rd28, %fd1801;
	xor.b64  	%rd29, %rd28, -9223372036854775808;
	mov.b64 	 %fd1801, %rd29;
	bra.uni 	BB0_74;

BB0_213:
	setp.eq.f64	%p179, %fd1846, 0dBFF0000000000000;
	@%p179 bra 	BB0_217;

	setp.gt.f64	%p180, %fd203, 0d3FF0000000000000;
	mov.f64 	%fd1564, 0d0000000000000000;
	rcp.rn.f64 	%fd1565, %fd1564;
	selp.f64	%fd1853, 0d0000000000000000, %fd1565, %p180;
	bra.uni 	BB0_217;

BB0_193:
	setp.geu.f64	%p164, %fd1846, 0d0000000000000000;
	@%p164 bra 	BB0_195;

	cvt.rzi.f64.f64	%fd1383, %fd1372;
	setp.neu.f64	%p165, %fd1383, 0dBFF0000000000000;
	mov.f64 	%fd1853, 0dFFF8000000000000;
	@%p165 bra 	BB0_217;

BB0_195:
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r400}, %fd203; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r399, hi}, %fd203; 
	}
	// inline asm
	bfe.u32 	%r401, %r400, 20, 11;
	setp.ne.s32	%p166, %r401, 0;
	@%p166 bra 	BB0_197;

	mov.f64 	%fd1388, 0d4350000000000000;
	mul.rn.f64 	%fd1387, %fd203, %fd1388;
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r400}, %fd1387; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r399, hi}, %fd1387; 
	}
	// inline asm
	bfe.u32 	%r300, %r400, 20, 11;
	add.s32 	%r401, %r300, -54;

BB0_197:
	add.s32 	%r402, %r401, -1023;
	and.b32  	%r303, %r400, -2146435073;
	or.b32  	%r302, %r303, 1072693248;
	// inline asm
	mov.b64 	%fd1849, {%r399, %r302};
	// inline asm
	setp.lt.u32	%p167, %r302, 1073127583;
	@%p167 bra 	BB0_199;

	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r304, hi}, %fd1849; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r305}, %fd1849; 
	}
	// inline asm
	add.s32 	%r307, %r305, -1048576;
	// inline asm
	mov.b64 	%fd1849, {%r304, %r307};
	// inline asm
	add.s32 	%r402, %r401, -1022;

BB0_199:
	add.f64 	%fd1477, %fd1849, 0d3FF0000000000000;
	rcp.rn.f64 	%fd1478, %fd1477;
	add.f64 	%fd1419, %fd1849, 0dBFF0000000000000;
	mul.rn.f64 	%fd1479, %fd1419, %fd1478;
	add.f64 	%fd1467, %fd1479, %fd1479;
	mul.rn.f64 	%fd1415, %fd1467, %fd1467;
	mov.f64 	%fd1394, 0d3EB0F5FF7D2CAFE2;
	mov.f64 	%fd1396, 0d3ED0F5D241AD3B5A;
	// inline asm
	fma.rn.f64 	%fd1393, %fd1394, %fd1415, %fd1396;
	// inline asm
	mov.f64 	%fd1400, 0d3EF3B20A75488A3F;
	// inline asm
	fma.rn.f64 	%fd1397, %fd1393, %fd1415, %fd1400;
	// inline asm
	mov.f64 	%fd1404, 0d3F1745CDE4FAECD5;
	// inline asm
	fma.rn.f64 	%fd1401, %fd1397, %fd1415, %fd1404;
	// inline asm
	mov.f64 	%fd1408, 0d3F3C71C7258A578B;
	// inline asm
	fma.rn.f64 	%fd1405, %fd1401, %fd1415, %fd1408;
	// inline asm
	mov.f64 	%fd1412, 0d3F6249249242B910;
	// inline asm
	fma.rn.f64 	%fd1409, %fd1405, %fd1415, %fd1412;
	// inline asm
	mov.f64 	%fd1416, 0d3F89999999999DFB;
	// inline asm
	fma.rn.f64 	%fd1413, %fd1409, %fd1415, %fd1416;
	// inline asm
	mul.rn.f64 	%fd1480, %fd1413, %fd1415;
	sub.f64 	%fd1481, %fd1419, %fd1467;
	mul.rn.f64 	%fd1420, %fd1378, %fd1481;
	neg.f64 	%fd1418, %fd1467;
	// inline asm
	fma.rn.f64 	%fd1417, %fd1418, %fd1419, %fd1420;
	// inline asm
	mul.rn.f64 	%fd1463, %fd1478, %fd1417;
	add.f64 	%fd1483, %fd1480, 0d3FB5555555555555;
	mov.f64 	%fd1484, 0d3FB5555555555555;
	sub.f64 	%fd1485, %fd1484, %fd1483;
	add.f64 	%fd1486, %fd1480, %fd1485;
	add.f64 	%fd1487, %fd1486, 0d0000000000000000;
	add.f64 	%fd1488, %fd1487, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd1430, %fd1483, %fd1488;
	sub.f64 	%fd1489, %fd1483, %fd1430;
	add.f64 	%fd1434, %fd1488, %fd1489;
	mul.rn.f64 	%fd1490, %fd1430, %fd1467;
	neg.f64 	%fd1424, %fd1490;
	// inline asm
	fma.rn.f64 	%fd1421, %fd1430, %fd1467, %fd1424;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1425, %fd1434, %fd1463, %fd1421;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1429, %fd1430, %fd1463, %fd1425;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1433, %fd1434, %fd1467, %fd1429;
	// inline asm
	add.f64 	%fd1446, %fd1490, %fd1433;
	sub.f64 	%fd1491, %fd1490, %fd1446;
	add.f64 	%fd1450, %fd1433, %fd1491;
	mul.rn.f64 	%fd1492, %fd1446, %fd1467;
	neg.f64 	%fd1440, %fd1492;
	// inline asm
	fma.rn.f64 	%fd1437, %fd1446, %fd1467, %fd1440;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1441, %fd1450, %fd1463, %fd1437;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1445, %fd1446, %fd1463, %fd1441;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1449, %fd1450, %fd1467, %fd1445;
	// inline asm
	add.f64 	%fd1462, %fd1492, %fd1449;
	sub.f64 	%fd1493, %fd1492, %fd1462;
	add.f64 	%fd1466, %fd1449, %fd1493;
	mul.rn.f64 	%fd1494, %fd1462, %fd1467;
	neg.f64 	%fd1456, %fd1494;
	// inline asm
	fma.rn.f64 	%fd1453, %fd1462, %fd1467, %fd1456;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1457, %fd1466, %fd1463, %fd1453;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1461, %fd1462, %fd1463, %fd1457;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1465, %fd1466, %fd1467, %fd1461;
	// inline asm
	add.f64 	%fd1495, %fd1494, %fd1465;
	sub.f64 	%fd1496, %fd1494, %fd1495;
	add.f64 	%fd1497, %fd1465, %fd1496;
	add.f64 	%fd1498, %fd1467, %fd1495;
	sub.f64 	%fd1499, %fd1467, %fd1498;
	add.f64 	%fd1500, %fd1495, %fd1499;
	add.f64 	%fd1501, %fd1497, %fd1500;
	add.f64 	%fd1502, %fd1463, %fd1501;
	add.f64 	%fd1503, %fd1498, %fd1502;
	sub.f64 	%fd1504, %fd1498, %fd1503;
	add.f64 	%fd1505, %fd1502, %fd1504;
	cvt.rn.f64.s32	%fd1506, %r402;
	mov.f64 	%fd1507, 0d3FE62E42FEFA3000;
	mul.rn.f64 	%fd1508, %fd1506, %fd1507;
	mov.f64 	%fd1509, 0d3D53DE6AF278ECE6;
	mul.rn.f64 	%fd1510, %fd1506, %fd1509;
	add.f64 	%fd1511, %fd1508, %fd1503;
	sub.f64 	%fd1512, %fd1508, %fd1511;
	add.f64 	%fd1513, %fd1503, %fd1512;
	add.f64 	%fd1514, %fd1505, %fd1513;
	add.f64 	%fd1515, %fd1510, %fd1514;
	add.f64 	%fd1470, %fd1511, %fd1515;
	sub.f64 	%fd1516, %fd1511, %fd1470;
	add.f64 	%fd1474, %fd1515, %fd1516;
	mul.rn.f64 	%fd1517, %fd1470, %fd1372;
	neg.f64 	%fd1472, %fd1517;
	// inline asm
	fma.rn.f64 	%fd1469, %fd1470, %fd1372, %fd1472;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1473, %fd1474, %fd1372, %fd1469;
	// inline asm
	add.f64 	%fd208, %fd1517, %fd1473;
	sub.f64 	%fd1518, %fd1517, %fd208;
	add.f64 	%fd209, %fd1473, %fd1518;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r127}, %fd208;
	}
	mov.b32 	 %f9, %r127;
	abs.f32 	%f10, %f9;
	setp.lt.f32	%p168, %f10, 0f40874911;
	@%p168 bra 	BB0_201;
	bra.uni 	BB0_200;

BB0_201:
	mov.f64 	%fd1522, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd1523, %fd208, %fd1522;
	mov.f64 	%fd1524, 0d4338000000000000;
	add.rn.f64 	%fd1525, %fd1523, %fd1524;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r128, %temp}, %fd1525;
	}
	mov.f64 	%fd1526, 0dC338000000000000;
	add.rn.f64 	%fd1527, %fd1525, %fd1526;
	mov.f64 	%fd1528, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd1529, %fd1527, %fd1528, %fd208;
	mov.f64 	%fd1530, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd1531, %fd1527, %fd1530, %fd1529;
	mov.f64 	%fd1532, 0d3E928AF3FCA213EA;
	mov.f64 	%fd1533, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd1534, %fd1533, %fd1531, %fd1532;
	mov.f64 	%fd1535, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd1536, %fd1534, %fd1531, %fd1535;
	mov.f64 	%fd1537, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd1538, %fd1536, %fd1531, %fd1537;
	mov.f64 	%fd1539, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd1540, %fd1538, %fd1531, %fd1539;
	mov.f64 	%fd1541, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd1542, %fd1540, %fd1531, %fd1541;
	mov.f64 	%fd1543, 0d3F81111111122322;
	fma.rn.f64 	%fd1544, %fd1542, %fd1531, %fd1543;
	mov.f64 	%fd1545, 0d3FA55555555502A1;
	fma.rn.f64 	%fd1546, %fd1544, %fd1531, %fd1545;
	mov.f64 	%fd1547, 0d3FC5555555555511;
	fma.rn.f64 	%fd1548, %fd1546, %fd1531, %fd1547;
	mov.f64 	%fd1549, 0d3FE000000000000B;
	fma.rn.f64 	%fd1550, %fd1548, %fd1531, %fd1549;
	mov.f64 	%fd1551, 0d3FF0000000000000;
	fma.rn.f64 	%fd1552, %fd1550, %fd1531, %fd1551;
	fma.rn.f64 	%fd1850, %fd1552, %fd1531, %fd1551;
	abs.s32 	%r308, %r128;
	setp.lt.s32	%p171, %r308, 1023;
	@%p171 bra 	BB0_203;
	bra.uni 	BB0_202;

BB0_203:
	shl.b32 	%r314, %r128, 20;
	add.s32 	%r403, %r314, 1072693248;
	bra.uni 	BB0_204;

BB0_51:
	setp.geu.f64	%p46, %fd1793, 0d0000000000000000;
	@%p46 bra 	BB0_53;

	cvt.rzi.f64.f64	%fd484, %fd473;
	setp.neu.f64	%p47, %fd484, 0dBFF0000000000000;
	mov.f64 	%fd1801, 0dFFF8000000000000;
	@%p47 bra 	BB0_74;

BB0_53:
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r374}, %fd50; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r373, hi}, %fd50; 
	}
	// inline asm
	bfe.u32 	%r375, %r374, 20, 11;
	setp.ne.s32	%p48, %r375, 0;
	@%p48 bra 	BB0_55;

	mov.f64 	%fd489, 0d4350000000000000;
	mul.rn.f64 	%fd488, %fd50, %fd489;
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r374}, %fd488; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r373, hi}, %fd488; 
	}
	// inline asm
	bfe.u32 	%r187, %r374, 20, 11;
	add.s32 	%r375, %r187, -54;

BB0_55:
	add.s32 	%r376, %r375, -1023;
	and.b32  	%r190, %r374, -2146435073;
	or.b32  	%r189, %r190, 1072693248;
	// inline asm
	mov.b64 	%fd1797, {%r373, %r189};
	// inline asm
	setp.lt.u32	%p49, %r189, 1073127583;
	@%p49 bra 	BB0_57;

	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r191, hi}, %fd1797; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r192}, %fd1797; 
	}
	// inline asm
	add.s32 	%r194, %r192, -1048576;
	// inline asm
	mov.b64 	%fd1797, {%r191, %r194};
	// inline asm
	add.s32 	%r376, %r375, -1022;

BB0_57:
	add.f64 	%fd578, %fd1797, 0d3FF0000000000000;
	rcp.rn.f64 	%fd579, %fd578;
	add.f64 	%fd520, %fd1797, 0dBFF0000000000000;
	mul.rn.f64 	%fd580, %fd520, %fd579;
	add.f64 	%fd568, %fd580, %fd580;
	mul.rn.f64 	%fd516, %fd568, %fd568;
	mov.f64 	%fd495, 0d3EB0F5FF7D2CAFE2;
	mov.f64 	%fd497, 0d3ED0F5D241AD3B5A;
	// inline asm
	fma.rn.f64 	%fd494, %fd495, %fd516, %fd497;
	// inline asm
	mov.f64 	%fd501, 0d3EF3B20A75488A3F;
	// inline asm
	fma.rn.f64 	%fd498, %fd494, %fd516, %fd501;
	// inline asm
	mov.f64 	%fd505, 0d3F1745CDE4FAECD5;
	// inline asm
	fma.rn.f64 	%fd502, %fd498, %fd516, %fd505;
	// inline asm
	mov.f64 	%fd509, 0d3F3C71C7258A578B;
	// inline asm
	fma.rn.f64 	%fd506, %fd502, %fd516, %fd509;
	// inline asm
	mov.f64 	%fd513, 0d3F6249249242B910;
	// inline asm
	fma.rn.f64 	%fd510, %fd506, %fd516, %fd513;
	// inline asm
	mov.f64 	%fd517, 0d3F89999999999DFB;
	// inline asm
	fma.rn.f64 	%fd514, %fd510, %fd516, %fd517;
	// inline asm
	mul.rn.f64 	%fd581, %fd514, %fd516;
	sub.f64 	%fd582, %fd520, %fd568;
	mul.rn.f64 	%fd521, %fd479, %fd582;
	neg.f64 	%fd519, %fd568;
	// inline asm
	fma.rn.f64 	%fd518, %fd519, %fd520, %fd521;
	// inline asm
	mul.rn.f64 	%fd564, %fd579, %fd518;
	add.f64 	%fd584, %fd581, 0d3FB5555555555555;
	mov.f64 	%fd585, 0d3FB5555555555555;
	sub.f64 	%fd586, %fd585, %fd584;
	add.f64 	%fd587, %fd581, %fd586;
	add.f64 	%fd588, %fd587, 0d0000000000000000;
	add.f64 	%fd589, %fd588, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd531, %fd584, %fd589;
	sub.f64 	%fd590, %fd584, %fd531;
	add.f64 	%fd535, %fd589, %fd590;
	mul.rn.f64 	%fd591, %fd531, %fd568;
	neg.f64 	%fd525, %fd591;
	// inline asm
	fma.rn.f64 	%fd522, %fd531, %fd568, %fd525;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd526, %fd535, %fd564, %fd522;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd530, %fd531, %fd564, %fd526;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd534, %fd535, %fd568, %fd530;
	// inline asm
	add.f64 	%fd547, %fd591, %fd534;
	sub.f64 	%fd592, %fd591, %fd547;
	add.f64 	%fd551, %fd534, %fd592;
	mul.rn.f64 	%fd593, %fd547, %fd568;
	neg.f64 	%fd541, %fd593;
	// inline asm
	fma.rn.f64 	%fd538, %fd547, %fd568, %fd541;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd542, %fd551, %fd564, %fd538;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd546, %fd547, %fd564, %fd542;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd550, %fd551, %fd568, %fd546;
	// inline asm
	add.f64 	%fd563, %fd593, %fd550;
	sub.f64 	%fd594, %fd593, %fd563;
	add.f64 	%fd567, %fd550, %fd594;
	mul.rn.f64 	%fd595, %fd563, %fd568;
	neg.f64 	%fd557, %fd595;
	// inline asm
	fma.rn.f64 	%fd554, %fd563, %fd568, %fd557;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd558, %fd567, %fd564, %fd554;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd562, %fd563, %fd564, %fd558;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd566, %fd567, %fd568, %fd562;
	// inline asm
	add.f64 	%fd596, %fd595, %fd566;
	sub.f64 	%fd597, %fd595, %fd596;
	add.f64 	%fd598, %fd566, %fd597;
	add.f64 	%fd599, %fd568, %fd596;
	sub.f64 	%fd600, %fd568, %fd599;
	add.f64 	%fd601, %fd596, %fd600;
	add.f64 	%fd602, %fd598, %fd601;
	add.f64 	%fd603, %fd564, %fd602;
	add.f64 	%fd604, %fd599, %fd603;
	sub.f64 	%fd605, %fd599, %fd604;
	add.f64 	%fd606, %fd603, %fd605;
	cvt.rn.f64.s32	%fd607, %r376;
	mov.f64 	%fd608, 0d3FE62E42FEFA3000;
	mul.rn.f64 	%fd609, %fd607, %fd608;
	mov.f64 	%fd610, 0d3D53DE6AF278ECE6;
	mul.rn.f64 	%fd611, %fd607, %fd610;
	add.f64 	%fd612, %fd609, %fd604;
	sub.f64 	%fd613, %fd609, %fd612;
	add.f64 	%fd614, %fd604, %fd613;
	add.f64 	%fd615, %fd606, %fd614;
	add.f64 	%fd616, %fd611, %fd615;
	add.f64 	%fd571, %fd612, %fd616;
	sub.f64 	%fd617, %fd612, %fd571;
	add.f64 	%fd575, %fd616, %fd617;
	mul.rn.f64 	%fd618, %fd571, %fd473;
	neg.f64 	%fd573, %fd618;
	// inline asm
	fma.rn.f64 	%fd570, %fd571, %fd473, %fd573;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd574, %fd575, %fd473, %fd570;
	// inline asm
	add.f64 	%fd55, %fd618, %fd574;
	sub.f64 	%fd619, %fd618, %fd55;
	add.f64 	%fd56, %fd574, %fd619;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r45}, %fd55;
	}
	mov.b32 	 %f3, %r45;
	abs.f32 	%f4, %f3;
	setp.lt.f32	%p50, %f4, 0f40874911;
	@%p50 bra 	BB0_59;
	bra.uni 	BB0_58;

BB0_59:
	mov.f64 	%fd623, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd624, %fd55, %fd623;
	mov.f64 	%fd625, 0d4338000000000000;
	add.rn.f64 	%fd626, %fd624, %fd625;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r46, %temp}, %fd626;
	}
	mov.f64 	%fd627, 0dC338000000000000;
	add.rn.f64 	%fd628, %fd626, %fd627;
	mov.f64 	%fd629, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd630, %fd628, %fd629, %fd55;
	mov.f64 	%fd631, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd632, %fd628, %fd631, %fd630;
	mov.f64 	%fd633, 0d3E928AF3FCA213EA;
	mov.f64 	%fd634, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd635, %fd634, %fd632, %fd633;
	mov.f64 	%fd636, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd637, %fd635, %fd632, %fd636;
	mov.f64 	%fd638, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd639, %fd637, %fd632, %fd638;
	mov.f64 	%fd640, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd641, %fd639, %fd632, %fd640;
	mov.f64 	%fd642, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd643, %fd641, %fd632, %fd642;
	mov.f64 	%fd644, 0d3F81111111122322;
	fma.rn.f64 	%fd645, %fd643, %fd632, %fd644;
	mov.f64 	%fd646, 0d3FA55555555502A1;
	fma.rn.f64 	%fd647, %fd645, %fd632, %fd646;
	mov.f64 	%fd648, 0d3FC5555555555511;
	fma.rn.f64 	%fd649, %fd647, %fd632, %fd648;
	mov.f64 	%fd650, 0d3FE000000000000B;
	fma.rn.f64 	%fd651, %fd649, %fd632, %fd650;
	mov.f64 	%fd652, 0d3FF0000000000000;
	fma.rn.f64 	%fd653, %fd651, %fd632, %fd652;
	fma.rn.f64 	%fd1798, %fd653, %fd632, %fd652;
	abs.s32 	%r195, %r46;
	setp.lt.s32	%p53, %r195, 1023;
	@%p53 bra 	BB0_61;
	bra.uni 	BB0_60;

BB0_61:
	shl.b32 	%r201, %r46, 20;
	add.s32 	%r377, %r201, 1072693248;
	bra.uni 	BB0_62;

BB0_200:
	setp.lt.s32	%p169, %r127, 0;
	selp.f64	%fd1519, 0d0000000000000000, 0d7FF0000000000000, %p169;
	abs.f64 	%fd1520, %fd208;
	setp.gtu.f64	%p170, %fd1520, 0d7FF0000000000000;
	add.f64 	%fd1521, %fd208, %fd208;
	selp.f64	%fd1853, %fd1521, %fd1519, %p170;
	bra.uni 	BB0_205;

BB0_70:
	setp.eq.f64	%p60, %fd1793, 0dBFF0000000000000;
	@%p60 bra 	BB0_74;

	setp.gt.f64	%p61, %fd50, 0d3FF0000000000000;
	mov.f64 	%fd662, 0d0000000000000000;
	rcp.rn.f64 	%fd663, %fd662;
	selp.f64	%fd1801, 0d0000000000000000, %fd663, %p61;
	bra.uni 	BB0_74;

BB0_202:
	add.s32 	%r309, %r128, 2046;
	shl.b32 	%r310, %r309, 19;
	and.b32  	%r311, %r310, -1048576;
	shl.b32 	%r312, %r309, 20;
	sub.s32 	%r403, %r312, %r311;
	mov.u32 	%r313, 0;
	mov.b64 	%fd1553, {%r313, %r311};
	mul.f64 	%fd1850, %fd1850, %fd1553;

BB0_204:
	mov.u32 	%r315, 0;
	mov.b64 	%fd1554, {%r315, %r403};
	mul.f64 	%fd1853, %fd1850, %fd1554;

BB0_205:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r316}, %fd1853;
	}
	and.b32  	%r317, %r316, 2147483647;
	setp.ne.s32	%p172, %r317, 2146435072;
	@%p172 bra 	BB0_207;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r318, %temp}, %fd1853;
	}
	setp.eq.s32	%p173, %r318, 0;
	@%p173 bra 	BB0_208;

BB0_207:
	// inline asm
	fma.rn.f64 	%fd1853, %fd1853, %fd209, %fd1853;
	// inline asm

BB0_208:
	setp.neu.f64	%p174, %fd204, 0d3FF0000000000000;
	or.pred  	%p176, %p164, %p174;
	@%p176 bra 	BB0_217;

	mov.b64 	 %rd56, %fd1853;
	xor.b64  	%rd57, %rd56, -9223372036854775808;
	mov.b64 	 %fd1853, %rd57;

BB0_217:
	mul.f64 	%fd226, %fd1845, %fd1853;
	mov.f64 	%fd1854, 0d0000000000000000;
	@%p139 bra 	BB0_220;

	ld.global.f64 	%fd227, [%rd4];
	abs.f64 	%fd1568, %fd227;
	setp.gtu.f64	%p183, %fd1568, 0d7FF0000000000000;
	@%p183 bra 	BB0_220;

	sub.f64 	%fd1569, %fd227, %fd226;
	fma.rn.f64 	%fd1854, %fd1569, %fd1569, 0d0000000000000000;

BB0_220:
	@%p139 bra 	BB0_223;

	ld.global.f64 	%fd230, [%rd5];
	abs.f64 	%fd1570, %fd230;
	setp.gtu.f64	%p185, %fd1570, 0d7FF0000000000000;
	@%p185 bra 	BB0_223;

	sub.f64 	%fd1571, %fd230, %fd226;
	fma.rn.f64 	%fd1854, %fd1571, %fd1571, %fd1854;

BB0_223:
	mov.f64 	%fd1861, 0dFFF8000000000000;
	setp.le.f64	%p186, %fd1846, 0d3FF0000000000000;
	@%p186 bra 	BB0_256;

	add.f64 	%fd233, %fd1846, 0dBFF0000000000000;
	setp.eq.f64	%p187, %fd233, 0d3FF0000000000000;
	mov.f64 	%fd1860, 0d3FF0000000000000;
	@%p187 bra 	BB0_255;

	abs.f64 	%fd234, %fd233;
	setp.gtu.f64	%p188, %fd234, 0d7FF0000000000000;
	@%p188 bra 	BB0_254;
	bra.uni 	BB0_226;

BB0_254:
	add.f64 	%fd1860, %fd233, 0dBFF0000000000000;
	bra.uni 	BB0_255;

BB0_226:
	setp.eq.f64	%p189, %fd233, 0d7FF0000000000000;
	mov.f64 	%fd1574, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r132}, %fd1574;
	}
	@%p189 bra 	BB0_253;
	bra.uni 	BB0_227;

BB0_253:
	setp.gt.s32	%p211, %r132, -1;
	selp.f64	%fd1860, 0d7FF0000000000000, 0d0000000000000000, %p211;
	bra.uni 	BB0_255;

BB0_227:
	and.b32  	%r319, %r132, 2147483647;
	setp.ne.s32	%p190, %r319, 2146435072;
	@%p190 bra 	BB0_229;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r320, %temp}, %fd1574;
	}
	setp.eq.s32	%p191, %r320, 0;
	@%p191 bra 	BB0_251;

BB0_229:
	mov.f64 	%fd1577, 0d3FE0000000000000;
	mul.rn.f64 	%fd1578, %fd1577, %fd1574;
	cvt.rzi.f64.f64	%fd1579, %fd1578;
	mov.f64 	%fd1580, 0d4000000000000000;
	mul.rn.f64 	%fd1581, %fd1580, %fd1579;
	sub.f64 	%fd1582, %fd1574, %fd1581;
	abs.f64 	%fd235, %fd1582;
	setp.eq.f64	%p192, %fd233, 0d0000000000000000;
	@%p192 bra 	BB0_250;
	bra.uni 	BB0_230;

BB0_250:
	setp.eq.f64	%p208, %fd235, 0d3FF0000000000000;
	rcp.rn.f64 	%fd1762, %fd233;
	mov.f64 	%fd1763, 0d0000000000000000;
	rcp.rn.f64 	%fd1764, %fd1763;
	selp.f64	%fd1860, %fd1762, %fd1764, %p208;
	bra.uni 	BB0_255;

BB0_230:
	setp.eq.f64	%p193, %fd233, 0dFFF0000000000000;
	@%p193 bra 	BB0_248;
	bra.uni 	BB0_231;

BB0_248:
	div.rn.f64 	%fd1860, %fd1574, %fd233;
	setp.neu.f64	%p207, %fd235, 0d3FF0000000000000;
	@%p207 bra 	BB0_255;

	mov.b64 	 %rd62, %fd1860;
	xor.b64  	%rd63, %rd62, -9223372036854775808;
	mov.b64 	 %fd1860, %rd63;
	bra.uni 	BB0_255;

BB0_251:
	setp.eq.f64	%p209, %fd233, 0dBFF0000000000000;
	@%p209 bra 	BB0_255;

	setp.gt.f64	%p210, %fd234, 0d3FF0000000000000;
	mov.f64 	%fd1766, 0d0000000000000000;
	rcp.rn.f64 	%fd1767, %fd1766;
	selp.f64	%fd1860, 0d0000000000000000, %fd1767, %p210;
	bra.uni 	BB0_255;

BB0_231:
	setp.geu.f64	%p194, %fd233, 0d0000000000000000;
	@%p194 bra 	BB0_233;

	cvt.rzi.f64.f64	%fd1585, %fd1574;
	setp.neu.f64	%p195, %fd1585, 0dBFF0000000000000;
	mov.f64 	%fd1860, 0dFFF8000000000000;
	@%p195 bra 	BB0_255;

BB0_233:
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r405}, %fd234; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r404, hi}, %fd234; 
	}
	// inline asm
	bfe.u32 	%r406, %r405, 20, 11;
	setp.ne.s32	%p196, %r406, 0;
	@%p196 bra 	BB0_235;

	mov.f64 	%fd1590, 0d4350000000000000;
	mul.rn.f64 	%fd1589, %fd234, %fd1590;
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r405}, %fd1589; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r404, hi}, %fd1589; 
	}
	// inline asm
	bfe.u32 	%r325, %r405, 20, 11;
	add.s32 	%r406, %r325, -54;

BB0_235:
	add.s32 	%r407, %r406, -1023;
	and.b32  	%r328, %r405, -2146435073;
	or.b32  	%r327, %r328, 1072693248;
	// inline asm
	mov.b64 	%fd1856, {%r404, %r327};
	// inline asm
	setp.lt.u32	%p197, %r327, 1073127583;
	@%p197 bra 	BB0_237;

	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r329, hi}, %fd1856; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r330}, %fd1856; 
	}
	// inline asm
	add.s32 	%r332, %r330, -1048576;
	// inline asm
	mov.b64 	%fd1856, {%r329, %r332};
	// inline asm
	add.s32 	%r407, %r406, -1022;

BB0_237:
	add.f64 	%fd1679, %fd1856, 0d3FF0000000000000;
	rcp.rn.f64 	%fd1680, %fd1679;
	add.f64 	%fd1621, %fd1856, 0dBFF0000000000000;
	mul.rn.f64 	%fd1681, %fd1621, %fd1680;
	add.f64 	%fd1669, %fd1681, %fd1681;
	mul.rn.f64 	%fd1617, %fd1669, %fd1669;
	mov.f64 	%fd1596, 0d3EB0F5FF7D2CAFE2;
	mov.f64 	%fd1598, 0d3ED0F5D241AD3B5A;
	// inline asm
	fma.rn.f64 	%fd1595, %fd1596, %fd1617, %fd1598;
	// inline asm
	mov.f64 	%fd1602, 0d3EF3B20A75488A3F;
	// inline asm
	fma.rn.f64 	%fd1599, %fd1595, %fd1617, %fd1602;
	// inline asm
	mov.f64 	%fd1606, 0d3F1745CDE4FAECD5;
	// inline asm
	fma.rn.f64 	%fd1603, %fd1599, %fd1617, %fd1606;
	// inline asm
	mov.f64 	%fd1610, 0d3F3C71C7258A578B;
	// inline asm
	fma.rn.f64 	%fd1607, %fd1603, %fd1617, %fd1610;
	// inline asm
	mov.f64 	%fd1614, 0d3F6249249242B910;
	// inline asm
	fma.rn.f64 	%fd1611, %fd1607, %fd1617, %fd1614;
	// inline asm
	mov.f64 	%fd1618, 0d3F89999999999DFB;
	// inline asm
	fma.rn.f64 	%fd1615, %fd1611, %fd1617, %fd1618;
	// inline asm
	mul.rn.f64 	%fd1682, %fd1615, %fd1617;
	sub.f64 	%fd1683, %fd1621, %fd1669;
	mul.rn.f64 	%fd1622, %fd1580, %fd1683;
	neg.f64 	%fd1620, %fd1669;
	// inline asm
	fma.rn.f64 	%fd1619, %fd1620, %fd1621, %fd1622;
	// inline asm
	mul.rn.f64 	%fd1665, %fd1680, %fd1619;
	add.f64 	%fd1685, %fd1682, 0d3FB5555555555555;
	mov.f64 	%fd1686, 0d3FB5555555555555;
	sub.f64 	%fd1687, %fd1686, %fd1685;
	add.f64 	%fd1688, %fd1682, %fd1687;
	add.f64 	%fd1689, %fd1688, 0d0000000000000000;
	add.f64 	%fd1690, %fd1689, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd1632, %fd1685, %fd1690;
	sub.f64 	%fd1691, %fd1685, %fd1632;
	add.f64 	%fd1636, %fd1690, %fd1691;
	mul.rn.f64 	%fd1692, %fd1632, %fd1669;
	neg.f64 	%fd1626, %fd1692;
	// inline asm
	fma.rn.f64 	%fd1623, %fd1632, %fd1669, %fd1626;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1627, %fd1636, %fd1665, %fd1623;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1631, %fd1632, %fd1665, %fd1627;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1635, %fd1636, %fd1669, %fd1631;
	// inline asm
	add.f64 	%fd1648, %fd1692, %fd1635;
	sub.f64 	%fd1693, %fd1692, %fd1648;
	add.f64 	%fd1652, %fd1635, %fd1693;
	mul.rn.f64 	%fd1694, %fd1648, %fd1669;
	neg.f64 	%fd1642, %fd1694;
	// inline asm
	fma.rn.f64 	%fd1639, %fd1648, %fd1669, %fd1642;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1643, %fd1652, %fd1665, %fd1639;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1647, %fd1648, %fd1665, %fd1643;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1651, %fd1652, %fd1669, %fd1647;
	// inline asm
	add.f64 	%fd1664, %fd1694, %fd1651;
	sub.f64 	%fd1695, %fd1694, %fd1664;
	add.f64 	%fd1668, %fd1651, %fd1695;
	mul.rn.f64 	%fd1696, %fd1664, %fd1669;
	neg.f64 	%fd1658, %fd1696;
	// inline asm
	fma.rn.f64 	%fd1655, %fd1664, %fd1669, %fd1658;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1659, %fd1668, %fd1665, %fd1655;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1663, %fd1664, %fd1665, %fd1659;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1667, %fd1668, %fd1669, %fd1663;
	// inline asm
	add.f64 	%fd1697, %fd1696, %fd1667;
	sub.f64 	%fd1698, %fd1696, %fd1697;
	add.f64 	%fd1699, %fd1667, %fd1698;
	add.f64 	%fd1700, %fd1669, %fd1697;
	sub.f64 	%fd1701, %fd1669, %fd1700;
	add.f64 	%fd1702, %fd1697, %fd1701;
	add.f64 	%fd1703, %fd1699, %fd1702;
	add.f64 	%fd1704, %fd1665, %fd1703;
	add.f64 	%fd1705, %fd1700, %fd1704;
	sub.f64 	%fd1706, %fd1700, %fd1705;
	add.f64 	%fd1707, %fd1704, %fd1706;
	cvt.rn.f64.s32	%fd1708, %r407;
	mov.f64 	%fd1709, 0d3FE62E42FEFA3000;
	mul.rn.f64 	%fd1710, %fd1708, %fd1709;
	mov.f64 	%fd1711, 0d3D53DE6AF278ECE6;
	mul.rn.f64 	%fd1712, %fd1708, %fd1711;
	add.f64 	%fd1713, %fd1710, %fd1705;
	sub.f64 	%fd1714, %fd1710, %fd1713;
	add.f64 	%fd1715, %fd1705, %fd1714;
	add.f64 	%fd1716, %fd1707, %fd1715;
	add.f64 	%fd1717, %fd1712, %fd1716;
	add.f64 	%fd1672, %fd1713, %fd1717;
	sub.f64 	%fd1718, %fd1713, %fd1672;
	add.f64 	%fd1676, %fd1717, %fd1718;
	mul.rn.f64 	%fd1719, %fd1672, %fd1574;
	neg.f64 	%fd1674, %fd1719;
	// inline asm
	fma.rn.f64 	%fd1671, %fd1672, %fd1574, %fd1674;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1675, %fd1676, %fd1574, %fd1671;
	// inline asm
	add.f64 	%fd239, %fd1719, %fd1675;
	sub.f64 	%fd1720, %fd1719, %fd239;
	add.f64 	%fd240, %fd1675, %fd1720;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r145}, %fd239;
	}
	mov.b32 	 %f11, %r145;
	abs.f32 	%f12, %f11;
	setp.lt.f32	%p198, %f12, 0f40874911;
	@%p198 bra 	BB0_239;
	bra.uni 	BB0_238;

BB0_239:
	mov.f64 	%fd1724, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd1725, %fd239, %fd1724;
	mov.f64 	%fd1726, 0d4338000000000000;
	add.rn.f64 	%fd1727, %fd1725, %fd1726;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r146, %temp}, %fd1727;
	}
	mov.f64 	%fd1728, 0dC338000000000000;
	add.rn.f64 	%fd1729, %fd1727, %fd1728;
	mov.f64 	%fd1730, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd1731, %fd1729, %fd1730, %fd239;
	mov.f64 	%fd1732, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd1733, %fd1729, %fd1732, %fd1731;
	mov.f64 	%fd1734, 0d3E928AF3FCA213EA;
	mov.f64 	%fd1735, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd1736, %fd1735, %fd1733, %fd1734;
	mov.f64 	%fd1737, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd1738, %fd1736, %fd1733, %fd1737;
	mov.f64 	%fd1739, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd1740, %fd1738, %fd1733, %fd1739;
	mov.f64 	%fd1741, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd1742, %fd1740, %fd1733, %fd1741;
	mov.f64 	%fd1743, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd1744, %fd1742, %fd1733, %fd1743;
	mov.f64 	%fd1745, 0d3F81111111122322;
	fma.rn.f64 	%fd1746, %fd1744, %fd1733, %fd1745;
	mov.f64 	%fd1747, 0d3FA55555555502A1;
	fma.rn.f64 	%fd1748, %fd1746, %fd1733, %fd1747;
	mov.f64 	%fd1749, 0d3FC5555555555511;
	fma.rn.f64 	%fd1750, %fd1748, %fd1733, %fd1749;
	mov.f64 	%fd1751, 0d3FE000000000000B;
	fma.rn.f64 	%fd1752, %fd1750, %fd1733, %fd1751;
	mov.f64 	%fd1753, 0d3FF0000000000000;
	fma.rn.f64 	%fd1754, %fd1752, %fd1733, %fd1753;
	fma.rn.f64 	%fd1857, %fd1754, %fd1733, %fd1753;
	abs.s32 	%r333, %r146;
	setp.lt.s32	%p201, %r333, 1023;
	@%p201 bra 	BB0_241;
	bra.uni 	BB0_240;

BB0_241:
	shl.b32 	%r339, %r146, 20;
	add.s32 	%r408, %r339, 1072693248;
	bra.uni 	BB0_242;

BB0_58:
	setp.lt.s32	%p51, %r45, 0;
	selp.f64	%fd620, 0d0000000000000000, 0d7FF0000000000000, %p51;
	abs.f64 	%fd621, %fd55;
	setp.gtu.f64	%p52, %fd621, 0d7FF0000000000000;
	add.f64 	%fd622, %fd55, %fd55;
	selp.f64	%fd1801, %fd622, %fd620, %p52;
	bra.uni 	BB0_63;

BB0_238:
	setp.lt.s32	%p199, %r145, 0;
	selp.f64	%fd1721, 0d0000000000000000, 0d7FF0000000000000, %p199;
	abs.f64 	%fd1722, %fd239;
	setp.gtu.f64	%p200, %fd1722, 0d7FF0000000000000;
	add.f64 	%fd1723, %fd239, %fd239;
	selp.f64	%fd1860, %fd1723, %fd1721, %p200;
	bra.uni 	BB0_243;

BB0_60:
	add.s32 	%r196, %r46, 2046;
	shl.b32 	%r197, %r196, 19;
	and.b32  	%r198, %r197, -1048576;
	shl.b32 	%r199, %r196, 20;
	sub.s32 	%r377, %r199, %r198;
	mov.u32 	%r200, 0;
	mov.b64 	%fd654, {%r200, %r198};
	mul.f64 	%fd1798, %fd1798, %fd654;

BB0_62:
	mov.u32 	%r202, 0;
	mov.b64 	%fd655, {%r202, %r377};
	mul.f64 	%fd1801, %fd1798, %fd655;

BB0_63:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r203}, %fd1801;
	}
	and.b32  	%r204, %r203, 2147483647;
	setp.ne.s32	%p54, %r204, 2146435072;
	@%p54 bra 	BB0_65;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r205, %temp}, %fd1801;
	}
	setp.eq.s32	%p55, %r205, 0;
	@%p55 bra 	BB0_66;

BB0_65:
	// inline asm
	fma.rn.f64 	%fd1801, %fd1801, %fd56, %fd1801;
	// inline asm

BB0_66:
	setp.neu.f64	%p56, %fd51, 0d3FF0000000000000;
	or.pred  	%p58, %p46, %p56;
	@%p58 bra 	BB0_74;

	mov.b64 	 %rd26, %fd1801;
	xor.b64  	%rd27, %rd26, -9223372036854775808;
	mov.b64 	 %fd1801, %rd27;
	bra.uni 	BB0_74;

BB0_240:
	add.s32 	%r334, %r146, 2046;
	shl.b32 	%r335, %r334, 19;
	and.b32  	%r336, %r335, -1048576;
	shl.b32 	%r337, %r334, 20;
	sub.s32 	%r408, %r337, %r336;
	mov.u32 	%r338, 0;
	mov.b64 	%fd1755, {%r338, %r336};
	mul.f64 	%fd1857, %fd1857, %fd1755;

BB0_242:
	mov.u32 	%r340, 0;
	mov.b64 	%fd1756, {%r340, %r408};
	mul.f64 	%fd1860, %fd1857, %fd1756;

BB0_243:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r341}, %fd1860;
	}
	and.b32  	%r342, %r341, 2147483647;
	setp.ne.s32	%p202, %r342, 2146435072;
	@%p202 bra 	BB0_245;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r343, %temp}, %fd1860;
	}
	setp.eq.s32	%p203, %r343, 0;
	@%p203 bra 	BB0_246;

BB0_245:
	// inline asm
	fma.rn.f64 	%fd1860, %fd1860, %fd240, %fd1860;
	// inline asm

BB0_246:
	setp.neu.f64	%p204, %fd235, 0d3FF0000000000000;
	or.pred  	%p206, %p194, %p204;
	@%p206 bra 	BB0_255;

	mov.b64 	 %rd60, %fd1860;
	xor.b64  	%rd61, %rd60, -9223372036854775808;
	mov.b64 	 %fd1860, %rd61;

BB0_255:
	mul.f64 	%fd1861, %fd1854, %fd1860;

BB0_256:
	ld.param.u64 	%rd69, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_0];
	add.f64 	%fd1768, %fd194, %fd1861;
	sub.f64 	%fd1769, %fd1768, %fd1840;
	add.f64 	%fd1770, %fd152, %fd1769;
	add.f64 	%fd1771, %fd1802, %fd1770;
	add.s64 	%rd65, %rd69, %rd54;
	st.global.f64 	[%rd65], %fd1771;
	ret;
}


  